{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Interview Transcript Semantic Chunker\n",
        "\n",
        "Created by [Matt Artz](https://www.mattartz.me/) ‚Äî Advancing AI Anthropology through computational approaches to qualitative research.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## What This Tool Does\n",
        "\n",
        "This notebook transforms lengthy interview transcripts into **semantically coherent chunks**‚Äîmeaningful segments that respect natural conversation boundaries while preserving speaker identity and turn-taking patterns. Rather than facing a 50-page interview transcript as one overwhelming document, you receive coherent segments, each labeled and sized for systematic qualitative coding.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "1. **Multi-Format Support**: Works with common transcript formats (PDF, DOCX, TXT, RTF)\n",
        "2. **Ethnographically-Aware Chunking**: Preserves speaker labels (Q:, A:, Interviewer:, etc.) and conversation structure\n",
        "3. **Intelligent Segmentation**: Uses AI to identify natural topic boundaries rather than arbitrary text splits\n",
        "4. **Two Analysis Methods**:\n",
        "   - **Claude API**: More sophisticated understanding of conversational flow\n",
        "   - **Sentence Transformers**: No API required, good for privacy-sensitive research\n",
        "5. **Flexible Configuration**: Adjust chunk sizes and sensitivity to match your analytical needs\n",
        "6. **Multiple Export Formats**: CSV, Excel, JSON - structured for use with NVivo, ATLAS.ti, and other analysis tools\n",
        "7. **Quality Metrics**: Built-in validation to ensure text preservation and chunking quality\n",
        "\n",
        "## Workflow\n",
        "\n",
        "1. **Configure Parameters**: Set chunking sensitivity and size preferences using intuitive controls\n",
        "2. **Upload Transcripts**: Batch process multiple interview files\n",
        "3. **AI-Powered Segmentation**: Automatic identification of topic boundaries and speaker turns\n",
        "4. **Quality Review**: Statistical analysis of chunking results with visualizations\n",
        "5. **Export for Analysis**: Download structured data ready for qualitative coding software\n",
        "\n",
        "## Applications in Anthropological Practice\n",
        "\n",
        "This tool supports any research involving interview transcripts‚Äîfrom dissertation fieldwork to applied research projects. It's particularly useful for computational analysis using the tools in my AI Anthropology Toolkit and comparative studies requiring standardized data units and collaborative research where multiple team members need consistent data preparation.\n",
        "\n",
        "## Methodological Positioning\n",
        "\n",
        "This tool represents a **computational anthropology** approach‚Äîusing AI to enhance rather than replace traditional ethnographic analysis. The chunking preserves the interpretive work that defines anthropological inquiry while addressing the practical challenges of scale in contemporary research contexts.\n",
        "\n",
        "**Important**: This tool prepares data for analysis but does not interpret it.\n",
        "\n",
        "## Target Audience\n",
        "\n",
        "Designed for anthropologists and qualitative researchers working with interview data‚Äîfrom graduate students managing thesis interviews to research teams processing large datasets for applied projects.\n",
        "\n",
        "## Technical Approach\n",
        "\n",
        "The notebook employs **semantic similarity analysis** to identify topical coherence in conversation. Rather than splitting text arbitrarily, it analyzes meaning relationships between sentences to find natural break points where topics shift or new themes emerge. This preserves the conversational integrity essential for anthropological interpretation.\n",
        "\n",
        "## Contributing to AI Anthropology\n",
        "\n",
        "This notebook contributes to the emerging field of AI Anthropology‚Äîwhich combines studying AI as cultural artifact, using AI to enhance ethnographic research, and applying anthropological insights to AI development (Artz, forthcoming). By open-sourcing these tools, this work advances the collective capacity of anthropologists to work effectively with computational methods.\n",
        "\n",
        "## AI Anthropology Toolkit\n",
        "\n",
        "This tool is part of a growing suite of computational resources for anthropological research:\n",
        "\n",
        "- **[Qualitative Codebook Builder](https://github.com/MattArtzAnthro/Qualitative_Codebook_Builder)** - AI-assisted development of qualitative coding frameworks\n",
        "- **[Interview Transcript Semantic Chunker](https://github.com/MattArtzAnthro/Interview_Transcript_Semantic_Chunker)** (this tool) - AI-assisted segmentation of interview transcripts\n",
        "- **[Coding and Thematic Analysis](https://github.com/MattArtzAnthro/Coding_and_Thematic_Analysis)** - AI-assisted coding and thematic analysis of qualtiative data\n",
        "\n",
        "*Additional tools will be added to this toolkit as they are developed.*\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) license. You may remix, adapt, and build upon the material for non-commercial purposes, provided you credit Matt Artz and link to the repository.\n",
        "\n",
        "**Full license details**: https://creativecommons.org/licenses/by-nc/4.0/\n",
        "\n",
        "## Attribution   \n",
        "\n",
        "If you use or adapt this project in your work, please cite:\n",
        "\n",
        "\n",
        "> Built with the Qualitative Codebook Builder (Matt Artz, 2025) ‚Äî https://github.com/MattArtzAnthro/Qualitative_Codebook_Builder\n",
        "\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this tool in your academic research, please cite:\n",
        "\n",
        "\n",
        "> Artz, Matt. 2025. Interview Transcript Semantic Chunker. Software.\n",
        "Zenodo. https://doi.org/10.5281/zenodo.15823716\n",
        "\n",
        "## Refrences\n",
        "Artz, Matt. Forthcoming. ‚ÄúAI Anthropology: The Future of Applied Anthropological Practice.‚Äù In Routledge Handbook of Applied Anthropology, edited by Christina Wasson, Edward B. Liebow, Karine L. Narahara, Ndukuyakhe Ndlovu, and Alaka Wali. New York: Routledge."
      ],
      "metadata": {
        "id": "oDxPthd7ZR5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Installation\n",
        "\n",
        "This section installs required Python packages and imports necessary libraries for semantic analysis, file processing, and interactive widgets. Run this cell first to ensure all dependencies are available.\n"
      ],
      "metadata": {
        "id": "MLgicFrP9cMi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctExxF7WZII7"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install python-docx pandas sentence-transformers nltk pypdf2 openpyxl ipywidgets matplotlib anthropic striprtf -q\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from typing import List, Tuple, Optional\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "import warnings\n",
        "import json\n",
        "from datetime import datetime\n",
        "import io\n",
        "from pathlib import Path\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import core libraries\n",
        "import docx\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import PyPDF2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download NLTK data for sentence tokenization\n",
        "print(\"Downloading NLTK data...\")\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "    print(\"‚úì NLTK data downloaded successfully\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è NLTK download issue - will try alternative tokenization\")\n",
        "\n",
        "print(\"‚úì All packages installed and libraries loaded successfully\")\n",
        "print(\"üìã Ready to configure your interview transcript chunker!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunking Configuration\n",
        "\n",
        "Configure your chunking parameters using interactive widgets to customize the analysis for your specific research needs. Choose between Claude API or local sentence transformers, adjust sensitivity thresholds, and set output preferences."
      ],
      "metadata": {
        "id": "OHwoKU7l9ogv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration Class and Interactive Interface\n",
        "\n",
        "class ChunkerConfig:\n",
        "    \"\"\"Configuration for semantic chunking following best practices\"\"\"\n",
        "\n",
        "    # Core chunking parameters\n",
        "    SIMILARITY_THRESHOLD = 0.5  # Lower = more chunks, Higher = fewer chunks\n",
        "    MAX_CHUNK_SENTENCES = 5     # Maximum sentences per chunk\n",
        "    MIN_CHUNK_SENTENCES = 1     # Minimum sentences per chunk\n",
        "    MODEL_NAME = 'all-MiniLM-L6-v2'  # Sentence transformer model\n",
        "    CHUNK_OVERLAP = 0           # Number of sentences to overlap between chunks\n",
        "\n",
        "    # Text processing options (all default to True)\n",
        "    PRESERVE_SPEAKER_LABELS = True\n",
        "    PRESERVE_TIMESTAMPS = True    # Keep timestamp patterns (True = keep them)\n",
        "    USE_PARAGRAPH_BOUNDARIES = True\n",
        "    MIN_SENTENCE_LENGTH = 5     # Minimum words in a sentence\n",
        "\n",
        "    # Chunking method\n",
        "    CHUNKING_METHOD = 'claude'  # 'embeddings' or 'claude' - Default to Claude\n",
        "    CLAUDE_API_KEY = ''\n",
        "    CLAUDE_MODEL = 'claude-4-sonnet-20250514'  # Default to Claude 4.0\n",
        "\n",
        "    # Output options\n",
        "    OUTPUT_FORMAT = 'excel'       # csv, excel, json - Default to Excel (recommended)\n",
        "    INCLUDE_METADATA = True\n",
        "    INCLUDE_SPEAKER_COLUMN = True\n",
        "    TIMESTAMP_OUTPUT = True\n",
        "\n",
        "# Create global config instance\n",
        "config = ChunkerConfig()\n",
        "\n",
        "def create_configuration_interface():\n",
        "    \"\"\"Create an interactive configuration interface using widgets\"\"\"\n",
        "\n",
        "    style = {'description_width': '160px'}\n",
        "    layout = widgets.Layout(width='420px')\n",
        "\n",
        "    # Instructions\n",
        "    instructions_html = \"\"\"\n",
        "    <div style='background-color: #E7ECEF; padding: 20px; border-radius: 10px; margin: 20px 0; border-left: 5px solid #274C77;'>\n",
        "    <h2 style='color: #274C77; margin-top: 0;'>üéØ Interview Transcript Semantic Chunker</h2>\n",
        "    <p><strong>Welcome!</strong> This tool helps social scientists process interview transcripts by breaking them into semantically coherent chunks for qualitative analysis.</p>\n",
        "    <h3 style='color: #274C77;'>üìñ How to Use:</h3>\n",
        "    <ol>\n",
        "        <li><strong>Configure:</strong> Adjust the settings below based on your research needs</li>\n",
        "        <li><strong>Upload:</strong> Add your interview transcript files (multiple formats supported)</li>\n",
        "        <li><strong>Process:</strong> Run the semantic chunking algorithm</li>\n",
        "        <li><strong>Export:</strong> Download your chunked data as CSV/Excel for coding</li>\n",
        "    </ol>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    # Core Parameters Section\n",
        "    core_header = widgets.HTML(\"<h3 style='margin: 20px 0 10px 0; color: #2c3e50;'>üéØ Core Chunking Parameters</h3>\")\n",
        "\n",
        "    similarity_slider = widgets.FloatSlider(\n",
        "        value=config.SIMILARITY_THRESHOLD,\n",
        "        min=0.1,\n",
        "        max=0.9,\n",
        "        step=0.05,\n",
        "        description='Similarity Threshold:',\n",
        "        disabled=False,\n",
        "        continuous_update=False,\n",
        "        readout=True,\n",
        "        readout_format='.2f',\n",
        "        style=style,\n",
        "        layout=layout\n",
        "    )\n",
        "\n",
        "    max_sentences_slider = widgets.IntSlider(\n",
        "        value=config.MAX_CHUNK_SENTENCES,\n",
        "        min=1,\n",
        "        max=10,\n",
        "        step=1,\n",
        "        description='Max Sentences:',\n",
        "        disabled=False,\n",
        "        continuous_update=False,\n",
        "        readout=True,\n",
        "        style=style,\n",
        "        layout=layout\n",
        "    )\n",
        "\n",
        "    min_sentences_slider = widgets.IntSlider(\n",
        "        value=config.MIN_CHUNK_SENTENCES,\n",
        "        min=1,\n",
        "        max=5,\n",
        "        step=1,\n",
        "        description='Min Sentences:',\n",
        "        disabled=False,\n",
        "        continuous_update=False,\n",
        "        readout=True,\n",
        "        style=style,\n",
        "        layout=layout\n",
        "    )\n",
        "\n",
        "    # Chunking Method Selection\n",
        "    method_header = widgets.HTML(\"<h3 style='margin: 30px 0 15px 0; color: #2c3e50;'>üß† Chunking Method</h3>\")\n",
        "\n",
        "    method_radio = widgets.RadioButtons(\n",
        "        options=[\n",
        "            ('Claude API (More Intelligent)', 'claude'),\n",
        "            ('Sentence Embeddings (No API Key)', 'embeddings')\n",
        "        ],\n",
        "        value=config.CHUNKING_METHOD,\n",
        "        description='Method:',\n",
        "        disabled=False,\n",
        "        style={'description_width': '80px'},\n",
        "        layout=widgets.Layout(width='450px', margin='10px 0')\n",
        "    )\n",
        "\n",
        "    api_key_text = widgets.Password(\n",
        "        value=config.CLAUDE_API_KEY,\n",
        "        placeholder='Enter Claude API key (only if using Claude method)',\n",
        "        description='API Key:',\n",
        "        disabled=False,\n",
        "        style=style,\n",
        "        layout=layout\n",
        "    )\n",
        "\n",
        "    # Claude model selection\n",
        "    claude_model_dropdown = widgets.Dropdown(\n",
        "        options=[\n",
        "            ('Claude 4.0 Sonnet', 'claude-4-sonnet-20250514'),\n",
        "            ('Claude 3.5 Sonnet (Latest)', 'claude-3-5-sonnet-20241022')\n",
        "        ],\n",
        "        value=config.CLAUDE_MODEL,\n",
        "        description='Claude Model:',\n",
        "        disabled=False,\n",
        "        style=style,\n",
        "        layout=layout\n",
        "    )\n",
        "\n",
        "    # Model Selection (for embeddings method)\n",
        "    model_header = widgets.HTML(\"<h3 style='margin: 20px 0 10px 0; color: #2c3e50;'>üîß Model Settings</h3>\")\n",
        "\n",
        "    model_dropdown = widgets.Dropdown(\n",
        "        options=[\n",
        "            ('Mini-LM (Fast, Good Quality)', 'all-MiniLM-L6-v2'),\n",
        "            ('MPNet (High Quality, Slower)', 'all-mpnet-base-v2'),\n",
        "            ('Multi-QA (Question-Answer)', 'multi-qa-MiniLM-L6-cos-v1'),\n",
        "        ],\n",
        "        value=config.MODEL_NAME,\n",
        "        description='Embedding Model:',\n",
        "        disabled=False,\n",
        "        style=style,\n",
        "        layout=layout\n",
        "    )\n",
        "\n",
        "    # Processing Options (Enhanced with timestamp removal)\n",
        "    processing_header = widgets.HTML(\"<h3 style='margin: 20px 0 10px 0; color: #2c3e50;'>üìù Text Processing Options</h3>\")\n",
        "\n",
        "    preserve_speakers_checkbox = widgets.Checkbox(\n",
        "        value=config.PRESERVE_SPEAKER_LABELS,\n",
        "        description='Preserve speaker labels (Q:, A:, Name:)',\n",
        "        disabled=False,\n",
        "        style={'description_width': 'initial'},\n",
        "        layout=widgets.Layout(width='400px', margin='5px 0')\n",
        "    )\n",
        "\n",
        "    preserve_timestamps_checkbox = widgets.Checkbox(\n",
        "        value=config.PRESERVE_TIMESTAMPS,\n",
        "        description='Preserve timestamps and time codes',\n",
        "        disabled=False,\n",
        "        style={'description_width': 'initial'},\n",
        "        layout=widgets.Layout(width='400px', margin='5px 0')\n",
        "    )\n",
        "\n",
        "    use_paragraphs_checkbox = widgets.Checkbox(\n",
        "        value=config.USE_PARAGRAPH_BOUNDARIES,\n",
        "        description='Respect paragraph boundaries',\n",
        "        disabled=False,\n",
        "        style={'description_width': 'initial'},\n",
        "        layout=widgets.Layout(width='400px', margin='5px 0')\n",
        "    )\n",
        "\n",
        "    # Output Settings\n",
        "    output_header = widgets.HTML(\"<h3 style='margin: 20px 0 10px 0; color: #2c3e50;'>üíæ Output Settings</h3>\")\n",
        "\n",
        "    output_format_radio = widgets.RadioButtons(\n",
        "        options=[\n",
        "            ('CSV - Simple spreadsheet (Excel/Google Sheets)', 'csv'),\n",
        "            ('Excel - Multi-sheet with statistics (Recommended)', 'excel'),\n",
        "            ('JSON - Structured data for programming', 'json')\n",
        "        ],\n",
        "        value=config.OUTPUT_FORMAT,\n",
        "        description='Output Format:',\n",
        "        disabled=False,\n",
        "        style={'description_width': '100px'},\n",
        "        layout=widgets.Layout(width='500px', margin='10px 0')\n",
        "    )\n",
        "\n",
        "    include_speaker_checkbox = widgets.Checkbox(\n",
        "        value=config.INCLUDE_SPEAKER_COLUMN,\n",
        "        description='Include speaker identification column',\n",
        "        disabled=False,\n",
        "        style={'description_width': 'initial'},\n",
        "        layout=widgets.Layout(width='400px', margin='5px 0')\n",
        "    )\n",
        "\n",
        "    # Action Buttons\n",
        "    buttons_header = widgets.HTML(\"<h3 style='margin: 30px 0 15px 0; color: #2c3e50;'>‚öôÔ∏è Actions</h3>\")\n",
        "\n",
        "    apply_button = widgets.Button(\n",
        "        description='‚úÖ Apply Configuration',\n",
        "        disabled=False,\n",
        "        button_style='',\n",
        "        tooltip='Apply current settings',\n",
        "        icon='check',\n",
        "        layout=widgets.Layout(width='220px', height='45px', margin='5px'),\n",
        "        style={'button_color': '#6096BA', 'font_weight': 'bold'}\n",
        "    )\n",
        "\n",
        "    test_button = widgets.Button(\n",
        "        description='üß™ Test Setup',\n",
        "        disabled=False,\n",
        "        button_style='',\n",
        "        tooltip='Test if everything is configured correctly',\n",
        "        icon='flask',\n",
        "        layout=widgets.Layout(width='160px', height='45px', margin='5px'),\n",
        "        style={'button_color': '#A3CEF1', 'font_weight': 'bold'}\n",
        "    )\n",
        "\n",
        "    # Status output\n",
        "    status_output = widgets.Output()\n",
        "\n",
        "    # Event handlers\n",
        "    def apply_configuration(b):\n",
        "        with status_output:\n",
        "            status_output.clear_output()\n",
        "            try:\n",
        "                # Update config with widget values\n",
        "                config.SIMILARITY_THRESHOLD = similarity_slider.value\n",
        "                config.MAX_CHUNK_SENTENCES = max_sentences_slider.value\n",
        "                config.MIN_CHUNK_SENTENCES = min_sentences_slider.value\n",
        "                config.MODEL_NAME = model_dropdown.value\n",
        "                config.CHUNKING_METHOD = method_radio.value\n",
        "                config.CLAUDE_API_KEY = api_key_text.value\n",
        "                config.CLAUDE_MODEL = claude_model_dropdown.value\n",
        "                config.PRESERVE_SPEAKER_LABELS = preserve_speakers_checkbox.value\n",
        "                config.PRESERVE_TIMESTAMPS = preserve_timestamps_checkbox.value\n",
        "                config.USE_PARAGRAPH_BOUNDARIES = use_paragraphs_checkbox.value\n",
        "                config.OUTPUT_FORMAT = output_format_radio.value\n",
        "                config.INCLUDE_SPEAKER_COLUMN = include_speaker_checkbox.value\n",
        "\n",
        "                print(\"‚úÖ Configuration applied successfully!\")\n",
        "                print(f\"üìä Method: {config.CHUNKING_METHOD}\")\n",
        "                print(f\"üìä Similarity Threshold: {config.SIMILARITY_THRESHOLD}\")\n",
        "                print(f\"üìù Chunk Size: {config.MIN_CHUNK_SENTENCES}-{config.MAX_CHUNK_SENTENCES} sentences\")\n",
        "                if config.CHUNKING_METHOD == 'embeddings':\n",
        "                    print(f\"üß† Model: {config.MODEL_NAME}\")\n",
        "                elif config.CHUNKING_METHOD == 'claude':\n",
        "                    print(f\"ü§ñ Claude Model: {config.CLAUDE_MODEL}\")\n",
        "                print(f\"üíæ Output: {config.OUTPUT_FORMAT.upper()}\")\n",
        "                print(f\"üîß Processing: Speaker labels {'preserved' if config.PRESERVE_SPEAKER_LABELS else 'removed'}, Timestamps {'preserved' if config.PRESERVE_TIMESTAMPS else 'removed'}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error applying configuration: {e}\")\n",
        "\n",
        "    def test_setup(b):\n",
        "        with status_output:\n",
        "            status_output.clear_output()\n",
        "            print(\"üß™ Testing setup...\")\n",
        "\n",
        "            if config.CHUNKING_METHOD == 'claude':\n",
        "                if not config.CLAUDE_API_KEY:\n",
        "                    print(\"‚ùå Claude API key required for Claude method\")\n",
        "                    return\n",
        "                print(f\"‚úÖ Claude API key provided\")\n",
        "                print(f\"ü§ñ Selected model: {config.CLAUDE_MODEL}\")\n",
        "                try:\n",
        "                    import anthropic\n",
        "                    print(\"‚úÖ Anthropic library available\")\n",
        "                except ImportError:\n",
        "                    print(\"‚ùå anthropic package not installed. Please install it: !pip install anthropic\")\n",
        "                    return\n",
        "            else:\n",
        "                try:\n",
        "                    print(f\"üîÑ Testing model: {config.MODEL_NAME}\")\n",
        "                    model = SentenceTransformer(config.MODEL_NAME)\n",
        "                    test_sentence = \"This is a test sentence.\"\n",
        "                    embedding = model.encode([test_sentence])\n",
        "                    print(f\"‚úÖ Model loaded successfully! Embedding dimension: {embedding.shape[1]}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Model loading failed: {e}\")\n",
        "                    return\n",
        "\n",
        "            print(\"‚úÖ All tests passed! Ready to process files.\")\n",
        "\n",
        "    # Bind events\n",
        "    apply_button.on_click(apply_configuration)\n",
        "    test_button.on_click(test_setup)\n",
        "\n",
        "    # Help documentation\n",
        "    help_html = \"\"\"\n",
        "    <div style='background-color: #A3CEF1; padding: 15px; border-radius: 5px; margin: 15px 0; border-left: 4px solid #6096BA;'>\n",
        "    <h4 style='color: #274C77; margin-top: 0;'>üìñ Configuration Guide</h4>\n",
        "    <div style='display: flex; gap: 20px; margin: 15px 0;'>\n",
        "        <div style='flex: 1;'>\n",
        "            <ul>\n",
        "                <li><strong>Similarity Threshold:</strong> Lower values (0.3-0.5) = more, shorter chunks. Higher values (0.6-0.8) = fewer, longer chunks.</li>\n",
        "                <li><strong>Chunking Methods:</strong>\n",
        "                    <ul>\n",
        "                        <li><em>Claude:</em> More intelligent chunking using Claude API (requires key) - recommended for best results</li>\n",
        "                        <li><em>Embeddings:</em> Uses AI sentence similarity (no API key needed) - good alternative option</li>\n",
        "                    </ul>\n",
        "                </li>\n",
        "                <li><strong>Claude Models:</strong>\n",
        "                    <ul>\n",
        "                        <li><em>4.0 Sonnet:</em> Latest model with enhanced capabilities (recommended)</li>\n",
        "                        <li><em>3.5 Sonnet:</em> Reliable fallback option with proven performance</li>\n",
        "                    </ul>\n",
        "                </li>\n",
        "                <li><strong>Text Processing:</strong>\n",
        "                    <ul>\n",
        "                        <li><em>Speaker Preservation:</em> Maintains \"Q:\", \"A:\", \"Interviewer:\" style labels and intelligently infers speakers for continuation paragraphs</li>\n",
        "                        <li><em>Timestamp Preservation:</em> Keeps [00:12:34] and similar time codes (uncheck to remove)</li>\n",
        "                        <li><em>Paragraph Boundaries:</em> Prevents chunks from crossing paragraph breaks</li>\n",
        "                    </ul>\n",
        "                </li>\n",
        "            </ul>\n",
        "        </div>\n",
        "        <div style='flex: 1;'>\n",
        "            <ul>\n",
        "                <li><strong>Export Formats:</strong>\n",
        "                    <ul>\n",
        "                        <li><em>CSV:</em> Simple spreadsheet - easy to open in Excel/Google Sheets for quick analysis</li>\n",
        "                        <li><em>Excel:</em> Professional format with 3 sheets (data + statistics + settings) - best for NVivo/ATLAS.ti import</li>\n",
        "                        <li><em>JSON:</em> Technical format with metadata - best for custom programming analysis</li>\n",
        "                    </ul>\n",
        "                </li>\n",
        "                <li><strong>File Storage:</strong>\n",
        "                    <ul>\n",
        "                        <li>All files saved to organized folders in Colab</li>\n",
        "                        <li>Includes README file explaining contents</li>\n",
        "                        <li>Configuration saved for reproducibility</li>\n",
        "                    </ul>\n",
        "                </li>\n",
        "            </ul>\n",
        "        </div>\n",
        "    </div>\n",
        "    <p><strong>üí° Tip:</strong> Start with default settings and Excel format for most research projects.</p>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    # Create organized layout sections\n",
        "    core_section = widgets.VBox([\n",
        "        core_header,\n",
        "        similarity_slider,\n",
        "        max_sentences_slider,\n",
        "        min_sentences_slider\n",
        "    ], layout=widgets.Layout(margin='10px'))\n",
        "\n",
        "    method_section = widgets.VBox([\n",
        "        method_header,\n",
        "        method_radio,\n",
        "        api_key_text,\n",
        "        claude_model_dropdown\n",
        "    ], layout=widgets.Layout(margin='10px'))\n",
        "\n",
        "    model_section = widgets.VBox([\n",
        "        model_header,\n",
        "        model_dropdown\n",
        "    ], layout=widgets.Layout(margin='10px'))\n",
        "\n",
        "    processing_section = widgets.VBox([\n",
        "        processing_header,\n",
        "        preserve_speakers_checkbox,\n",
        "        preserve_timestamps_checkbox,\n",
        "        use_paragraphs_checkbox\n",
        "    ], layout=widgets.Layout(margin='10px'))\n",
        "\n",
        "    output_section = widgets.VBox([\n",
        "        output_header,\n",
        "        output_format_radio,\n",
        "        include_speaker_checkbox\n",
        "    ], layout=widgets.Layout(margin='10px'))\n",
        "\n",
        "    buttons_section = widgets.VBox([\n",
        "        buttons_header,\n",
        "        widgets.HBox([apply_button, test_button])\n",
        "    ], layout=widgets.Layout(margin='10px'))\n",
        "\n",
        "    # Organize into two columns for better spacing\n",
        "    left_column = widgets.VBox([\n",
        "        core_section,\n",
        "        method_section,\n",
        "        model_section\n",
        "    ])\n",
        "\n",
        "    right_column = widgets.VBox([\n",
        "        processing_section,\n",
        "        output_section,\n",
        "        buttons_section\n",
        "    ])\n",
        "\n",
        "    main_container = widgets.VBox([\n",
        "        widgets.HBox([left_column, right_column], layout=widgets.Layout(gap='40px')),\n",
        "        status_output\n",
        "    ])\n",
        "\n",
        "    # Display everything\n",
        "    display(HTML(instructions_html))\n",
        "    display(HTML(help_html))\n",
        "    display(main_container)\n",
        "\n",
        "    return {\n",
        "        'apply_button': apply_button,\n",
        "        'test_button': test_button,\n",
        "        'status_output': status_output\n",
        "    }\n",
        "\n",
        "# Initialize configuration interface\n",
        "print(\"üéõÔ∏è Loading Configuration Interface...\")\n",
        "config_widgets = create_configuration_interface()"
      ],
      "metadata": {
        "id": "Z3q-E70J9ioc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File Upload and Document Processing\n",
        "\n",
        "Upload your interview transcript files in multiple formats (PDF, DOCX, TXT, RTF) and extract clean text for analysis. The notebook handles encoding issues and provides detailed file statistics.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Kkj1nU91_2fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File Upload and Document Processing\n",
        "\n",
        "def read_pdf(file_content):\n",
        "    \"\"\"Extract text from PDF file\"\"\"\n",
        "    try:\n",
        "        pdf_reader = PyPDF2.PdfReader(io.BytesIO(file_content))\n",
        "        text = []\n",
        "        for page in pdf_reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text.append(page_text)\n",
        "        return '\\n\\n'.join(text)\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error reading PDF: {e}\")\n",
        "\n",
        "def read_word_document(file_content):\n",
        "    \"\"\"Extract text from Word document\"\"\"\n",
        "    try:\n",
        "        doc = docx.Document(io.BytesIO(file_content))\n",
        "        full_text = []\n",
        "\n",
        "        for paragraph in doc.paragraphs:\n",
        "            if paragraph.text.strip():\n",
        "                full_text.append(paragraph.text.strip())\n",
        "\n",
        "        return '\\n\\n'.join(full_text)\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error reading Word document: {e}\")\n",
        "\n",
        "def read_text_file(file_content, encoding='utf-8'):\n",
        "    \"\"\"Read plain text file with fallback encoding detection\"\"\"\n",
        "    encodings_to_try = [encoding, 'latin-1', 'utf-16', 'cp1252', 'iso-8859-1']\n",
        "\n",
        "    for enc in encodings_to_try:\n",
        "        try:\n",
        "            return file_content.decode(enc)\n",
        "        except UnicodeDecodeError:\n",
        "            continue\n",
        "\n",
        "    raise ValueError(\"Could not decode file with any common encoding\")\n",
        "\n",
        "def read_rtf_file(file_content):\n",
        "    \"\"\"Read RTF file\"\"\"\n",
        "    try:\n",
        "        # Try to use striprtf if available\n",
        "        from striprtf.striprtf import rtf_to_text\n",
        "        rtf_text = file_content.decode('utf-8', errors='ignore')\n",
        "        return rtf_to_text(rtf_text)\n",
        "    except ImportError:\n",
        "        # Fallback: basic RTF stripping\n",
        "        rtf_text = file_content.decode('utf-8', errors='ignore')\n",
        "        # Simple RTF tag removal (basic fallback)\n",
        "        text = re.sub(r'\\\\[a-z]+\\d*\\s?', '', rtf_text)\n",
        "        text = re.sub(r'[{}]', '', text)\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error reading RTF file: {e}\")\n",
        "\n",
        "def create_file_upload_interface():\n",
        "    \"\"\"Create streamlined file upload interface\"\"\"\n",
        "\n",
        "    # Instructions\n",
        "    instructions_html = \"\"\"\n",
        "    <div style='background-color: #E7ECEF; padding: 20px; border-radius: 10px; margin: 20px 0; border-left: 5px solid #274C77;'>\n",
        "    <h3 style='color: #274C77; margin-top: 0;'>üìÅ Upload Your Interview Transcript(s)</h3>\n",
        "    <p><strong>Ready to upload your files!</strong> This tool supports multiple file formats commonly used for interview transcripts.</p>\n",
        "\n",
        "    <div style='display: flex; gap: 20px; margin: 15px 0;'>\n",
        "        <div style='flex: 1; background-color: #A3CEF1; padding: 15px; border-radius: 8px; border-left: 4px solid #6096BA;'>\n",
        "            <h4 style='color: #274C77; margin-top: 0;'>‚úÖ Supported Formats:</h4>\n",
        "            <ul>\n",
        "                <li><strong>.docx</strong> - Microsoft Word documents</li>\n",
        "                <li><strong>.pdf</strong> - PDF files (text-based only)</li>\n",
        "                <li><strong>.txt</strong> - Plain text files</li>\n",
        "                <li><strong>.rtf</strong> - Rich Text Format</li>\n",
        "            </ul>\n",
        "        </div>\n",
        "        <div style='flex: 1; background-color: #A3CEF1; padding: 15px; border-radius: 8px; border-left: 4px solid #6096BA;'>\n",
        "            <h4 style='color: #274C77; margin-top: 0;'>üí° Best Practices:</h4>\n",
        "            <ul>\n",
        "                <li>Use consistent speaker labels (Q:, A:, Name:)</li>\n",
        "                <li>Remove headers/footers/page numbers</li>\n",
        "                <li>One speaker turn per paragraph works best</li>\n",
        "                <li>Ensure text is selectable (not scanned images)</li>\n",
        "            </ul>\n",
        "        </div>\n",
        "    </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    # Upload button\n",
        "    upload_button = widgets.Button(\n",
        "        description='üì§ Choose Files to Upload',\n",
        "        disabled=False,\n",
        "        button_style='',\n",
        "        tooltip='Click to select and upload your interview transcript files',\n",
        "        icon='upload',\n",
        "        layout=widgets.Layout(width='300px', height='50px'),\n",
        "        style={'button_color': '#6096BA', 'font_weight': 'bold'}\n",
        "    )\n",
        "\n",
        "    # Clear button\n",
        "    clear_button = widgets.Button(\n",
        "        description='üóëÔ∏è Clear All Files',\n",
        "        disabled=False,\n",
        "        button_style='',\n",
        "        tooltip='Remove all uploaded files',\n",
        "        icon='trash',\n",
        "        layout=widgets.Layout(width='150px', height='40px'),\n",
        "        style={'button_color': '#8B8C89', 'font_weight': 'bold'}\n",
        "    )\n",
        "\n",
        "    # File info display\n",
        "    file_info = widgets.HTML(\n",
        "        value=\"<p style='color: #666; font-style: italic;'>No files uploaded yet. Click 'Choose Files to Upload' to begin.</p>\",\n",
        "        layout=widgets.Layout(width='100%')\n",
        "    )\n",
        "\n",
        "    # Upload progress/status\n",
        "    upload_output = widgets.Output()\n",
        "\n",
        "    # Global storage for uploaded files\n",
        "    global uploaded_files, file_stats\n",
        "    uploaded_files = {}\n",
        "    file_stats = {}\n",
        "\n",
        "    def handle_file_upload(b):\n",
        "        with upload_output:\n",
        "            upload_output.clear_output()\n",
        "\n",
        "            print(\"=\" * 60)\n",
        "            print(\"üìÅ FILE UPLOAD\")\n",
        "            print(\"=\" * 60)\n",
        "            print(\"Supported formats: .docx, .pdf, .txt, .rtf\")\n",
        "            print(\"Please select your interview transcript file(s)...\")\n",
        "            print()\n",
        "\n",
        "            try:\n",
        "                uploaded = files.upload()\n",
        "\n",
        "                if not uploaded:\n",
        "                    print(\"‚ùå No files were selected.\")\n",
        "                    return\n",
        "\n",
        "                print(f\"\\\\nüì• Processing {len(uploaded)} file(s)...\")\n",
        "\n",
        "                global uploaded_files, file_stats\n",
        "                uploaded_files.clear()\n",
        "                file_stats.clear()\n",
        "\n",
        "                successful_files = []\n",
        "                failed_files = []\n",
        "\n",
        "                for filename, file_content in uploaded.items():\n",
        "                    print(f\"\\\\nüîÑ Processing: {filename}\")\n",
        "\n",
        "                    try:\n",
        "                        # Determine file type and process\n",
        "                        file_ext = os.path.splitext(filename)[1].lower()\n",
        "\n",
        "                        if file_ext == '.docx':\n",
        "                            text = read_word_document(file_content)\n",
        "                        elif file_ext == '.pdf':\n",
        "                            text = read_pdf(file_content)\n",
        "                        elif file_ext in ['.txt']:\n",
        "                            text = read_text_file(file_content)\n",
        "                        elif file_ext == '.rtf':\n",
        "                            text = read_rtf_file(file_content)\n",
        "                        else:\n",
        "                            print(f\"   ‚ùå Unsupported file format: {file_ext}\")\n",
        "                            failed_files.append(filename)\n",
        "                            continue\n",
        "\n",
        "                        # Validate extracted text\n",
        "                        if not text or len(text.strip()) < 10:\n",
        "                            print(f\"   ‚ùå No meaningful text extracted from {filename}\")\n",
        "                            failed_files.append(filename)\n",
        "                            continue\n",
        "\n",
        "                        # Store file and calculate stats\n",
        "                        uploaded_files[filename] = text\n",
        "\n",
        "                        word_count = len(text.split())\n",
        "                        char_count = len(text)\n",
        "                        estimated_sentences = len([s for s in re.split(r'[.!?]+', text) if s.strip()])\n",
        "\n",
        "                        file_stats[filename] = {\n",
        "                            'word_count': word_count,\n",
        "                            'char_count': char_count,\n",
        "                            'sentence_count': estimated_sentences,\n",
        "                            'file_size': len(file_content)\n",
        "                        }\n",
        "\n",
        "                        print(f\"   ‚úÖ Success!\")\n",
        "                        print(f\"      üìä {word_count:,} words\")\n",
        "                        print(f\"      üìä {char_count:,} characters\")\n",
        "                        print(f\"      üìä ~{estimated_sentences:,} sentences\")\n",
        "                        print(f\"      üìÅ File size: {len(file_content):,} bytes\")\n",
        "\n",
        "                        successful_files.append(filename)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"   ‚ùå Error processing {filename}: {e}\")\n",
        "                        failed_files.append(filename)\n",
        "\n",
        "                # Update display\n",
        "                if successful_files:\n",
        "                    print(f\"\\\\nüéâ Successfully processed {len(successful_files)} file(s)!\")\n",
        "\n",
        "                    # Create summary display\n",
        "                    info_html = \"<div style='background-color: #A3CEF1; padding: 15px; border-radius: 5px; border-left: 4px solid #6096BA;'>\"\n",
        "                    info_html += f\"<h4 style='color: #274C77; margin-top: 0;'>‚úÖ {len(successful_files)} File(s) Successfully Uploaded</h4>\"\n",
        "\n",
        "                    for filename in successful_files:\n",
        "                        stats = file_stats[filename]\n",
        "                        info_html += f\"<div style='margin: 10px 0; padding: 10px; background-color: #E7ECEF; border-radius: 3px; border-left: 3px solid #274C77;'>\"\n",
        "                        info_html += f\"<strong>üìÑ {filename}</strong><br>\"\n",
        "                        info_html += f\"<small style='color: #274C77;'>üìä {stats['word_count']:,} words ‚Ä¢ {stats['sentence_count']:,} sentences ‚Ä¢ {stats['file_size']:,} bytes</small>\"\n",
        "                        info_html += \"</div>\"\n",
        "\n",
        "                    if len(successful_files) > 1:\n",
        "                        total_words = sum(stats['word_count'] for stats in file_stats.values())\n",
        "                        info_html += f\"<p style='color: #274C77;'><strong>üìà Total: {total_words:,} words across all files</strong></p>\"\n",
        "\n",
        "                    info_html += \"<p style='color: #274C77;'><strong>‚úÖ Ready to process! Continue to the next step.</strong></p>\"\n",
        "                    info_html += \"</div>\"\n",
        "\n",
        "                else:\n",
        "                    info_html = \"<div style='background-color: #E7ECEF; padding: 15px; border-radius: 5px; border-left: 4px solid #8B8C89;'>\"\n",
        "                    info_html += \"<h4 style='color: #274C77; margin-top: 0;'>‚ùå No Files Successfully Processed</h4>\"\n",
        "                    info_html += \"<p style='color: #274C77;'>Please check your file formats and try again.</p>\"\n",
        "                    info_html += \"</div>\"\n",
        "\n",
        "                if failed_files:\n",
        "                    print(f\"\\\\n‚ö†Ô∏è Failed to process {len(failed_files)} file(s): {', '.join(failed_files)}\")\n",
        "\n",
        "                file_info.value = info_html\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\\\n‚ùå Upload error: {e}\")\n",
        "                file_info.value = f\"<p style='color: red;'>‚ùå Upload failed: {e}</p>\"\n",
        "\n",
        "    def clear_files(b):\n",
        "        with upload_output:\n",
        "            upload_output.clear_output()\n",
        "\n",
        "            global uploaded_files, file_stats\n",
        "            uploaded_files.clear()\n",
        "            file_stats.clear()\n",
        "\n",
        "            file_info.value = \"<p style='color: #666; font-style: italic;'>Files cleared. Click 'Choose Files to Upload' to start over.</p>\"\n",
        "            print(\"üóëÔ∏è All files cleared successfully.\")\n",
        "\n",
        "    # Bind events\n",
        "    upload_button.on_click(handle_file_upload)\n",
        "    clear_button.on_click(clear_files)\n",
        "\n",
        "    # Layout\n",
        "    buttons_container = widgets.HBox([upload_button, clear_button])\n",
        "\n",
        "    # Display interface\n",
        "    display(HTML(instructions_html))\n",
        "    display(buttons_container)\n",
        "    display(file_info)\n",
        "    display(upload_output)\n",
        "\n",
        "    return {\n",
        "        'upload_button': upload_button,\n",
        "        'clear_button': clear_button,\n",
        "        'file_info': file_info,\n",
        "        'output': upload_output\n",
        "    }\n",
        "\n",
        "# Initialize file upload interface\n",
        "print(\"üìÅ File Upload Interface Ready\")\n",
        "print(\"üëÜ Configure your settings above, then upload your files below!\")\n",
        "\n",
        "# Global variables for file storage\n",
        "uploaded_files = {}\n",
        "file_stats = {}\n",
        "\n",
        "# Create and display upload interface\n",
        "upload_interface = create_file_upload_interface()"
      ],
      "metadata": {
        "id": "Pnusk3vQ_108"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Processing and Chunking\n",
        "\n",
        "Core functions that perform semantic analysis to identify natural conversation boundaries and create meaningful chunks. This section preserves speaker labels while intelligently segmenting content based on topic shifts."
      ],
      "metadata": {
        "id": "eSLBhoAbAaIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Core Text Processing and Chunking Functions\n",
        "\n",
        "def remove_timestamps(text):\n",
        "    \"\"\"Remove common timestamp patterns from interview transcripts\"\"\"\n",
        "    if config.PRESERVE_TIMESTAMPS:  # If preserving timestamps, don't remove them\n",
        "        return text\n",
        "\n",
        "    # First, handle timestamps within speaker labels\n",
        "    # Pattern for **Speaker: timestamp S#:** format\n",
        "    text = re.sub(r'(\\*\\*[^:*]+):\\s*\\d{1,2}:\\d{2}:\\d{2}\\.\\d+\\s*S\\d+:(\\*\\*)', r'\\1:\\2', text)\n",
        "    text = re.sub(r'(\\*\\*[^:*]+):\\s*\\d{1,2}:\\d{2}\\.\\d+\\s*S\\d+:(\\*\\*)', r'\\1:\\2', text)\n",
        "\n",
        "    # Common timestamp patterns to remove\n",
        "    timestamp_patterns = [\n",
        "        r'\\[\\d{1,2}:\\d{2}:\\d{2}\\]',           # [00:12:34]\n",
        "        r'\\[\\d{1,2}:\\d{2}\\]',                 # [12:34]\n",
        "        r'\\(\\d{1,2}:\\d{2}:\\d{2}\\)',           # (00:12:34)\n",
        "        r'\\(\\d{1,2}:\\d{2}\\)',                 # (12:34)\n",
        "        r'\\d{1,2}:\\d{2}:\\d{2}\\.\\d+\\s*S\\d+',   # 0:02:05.8 S2\n",
        "        r'\\d{1,2}:\\d{2}:\\d{2}\\s*S\\d+',        # 0:02:05 S2\n",
        "        r'\\d{1,2}:\\d{2}:\\d{2}\\.\\d+',          # 00:12:34.5\n",
        "        r'\\d{1,2}:\\d{2}:\\d{2}',               # 00:12:34\n",
        "        r'^\\d{1,2}:\\d{2}\\s',                  # 12:34 at start of line\n",
        "        r'<\\d{1,2}:\\d{2}:\\d{2}>',             # <00:12:34>\n",
        "        r'<\\d{1,2}:\\d{2}>',                   # <12:34>\n",
        "        r'\\*\\d{1,2}:\\d{2}:\\d{2}\\*',           # *00:12:34*\n",
        "        r'\\*\\d{1,2}:\\d{2}\\*',                 # *12:34*\n",
        "        r'Timestamp:\\s*\\d{1,2}:\\d{2}:\\d{2}',  # Timestamp: 00:12:34\n",
        "        r'Time:\\s*\\d{1,2}:\\d{2}:\\d{2}',       # Time: 00:12:34\n",
        "    ]\n",
        "\n",
        "    cleaned_text = text\n",
        "    for pattern in timestamp_patterns:\n",
        "        cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.MULTILINE)\n",
        "\n",
        "    # Clean up extra whitespace left by timestamp removal\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Multiple spaces to single\n",
        "    cleaned_text = re.sub(r'\\n\\s*\\n', '\\n\\n', cleaned_text)  # Clean up paragraph breaks\n",
        "\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "def extract_speaker_label(text):\n",
        "    \"\"\"Extract speaker label from text if present with enhanced pattern recognition\"\"\"\n",
        "    # Enhanced patterns for interview transcripts\n",
        "    patterns = [\n",
        "        r'^\\*\\*([^:*]+):\\*\\*\\s*(.*)',         # **Speaker:** format (markdown bold)\n",
        "        r'^\\*\\*([^:*]+):\\s*[\\d:.]+ S\\d+:\\*\\*\\s*(.*)',  # **Speaker: 0:02:05.8 S2:** format\n",
        "        r'^([A-Za-z0-9\\s]+):\\s*(.*)',         # Standard \"Speaker: text\"\n",
        "        r'^(Q|A):\\s*(.*)',             # \"Q: text\" or \"A: text\"\n",
        "        r'^\\[([^\\]]+)\\]:\\s*(.*)',      # \"[Speaker]: text\"\n",
        "        r'^([A-Z][a-z]*)\\s*[-‚Äì‚Äî]\\s*(.*)',  # \"Speaker - text\"\n",
        "        r'^(Interviewer|Participant|Researcher|Subject|Respondent)\\s*[:\\-‚Äì‚Äî]\\s*(.*)',  # Common interview roles\n",
        "        r'^([A-Z]{2,})\\s*[:\\-‚Äì‚Äî]\\s*(.*)',  # Acronym speakers like \"CEO:\", \"HR:\"\n",
        "        r'^\\*([^*]+)\\*:\\s*(.*)',       # \"*Speaker*: text\"\n",
        "        r'^(\\d+)\\.\\s*([A-Za-z][^:]*?):\\s*(.*)',  # \"1. Speaker: text\"\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = re.match(pattern, text.strip(), re.IGNORECASE)\n",
        "        if match:\n",
        "            groups = match.groups()\n",
        "            if len(groups) == 2:\n",
        "                return groups[0].strip(), groups[1].strip()\n",
        "            elif len(groups) == 3:  # Pattern with numbering\n",
        "                return groups[1].strip(), groups[2].strip()\n",
        "\n",
        "    return None, text\n",
        "\n",
        "def smart_speaker_inference(paragraphs):\n",
        "    \"\"\"Infer speakers for unlabeled paragraphs using context and patterns\"\"\"\n",
        "    processed_paragraphs = []\n",
        "    current_speaker = None\n",
        "    last_explicit_speaker = None\n",
        "    interview_pattern_detected = False\n",
        "\n",
        "    # Detect if this looks like a Q&A interview or uses markdown bold speakers\n",
        "    total_paragraphs = len(paragraphs)\n",
        "    q_a_count = sum(1 for para in paragraphs if re.match(r'^\\s*[QA]:', para, re.IGNORECASE))\n",
        "    bold_speaker_count = sum(1 for para in paragraphs if re.match(r'^\\*\\*[^:*]+:\\*\\*', para))\n",
        "\n",
        "    if q_a_count > total_paragraphs * 0.3:  # More than 30% are Q: or A:\n",
        "        interview_pattern_detected = True\n",
        "        print(\"üé§ Q&A interview pattern detected - enhanced speaker inference enabled\")\n",
        "    elif bold_speaker_count > total_paragraphs * 0.3:\n",
        "        print(\"üé§ Markdown bold speaker pattern detected\")\n",
        "\n",
        "    for i, para in enumerate(paragraphs):\n",
        "        if not para.strip():\n",
        "            continue\n",
        "\n",
        "        lines = para.split('\\n')\n",
        "        processed_lines = []\n",
        "        para_speaker = None\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            speaker, content = extract_speaker_label(line)\n",
        "\n",
        "            if speaker:\n",
        "                # Explicit speaker found\n",
        "                current_speaker = speaker\n",
        "                last_explicit_speaker = speaker\n",
        "                para_speaker = speaker\n",
        "                if content.strip():\n",
        "                    processed_lines.append(f\"{speaker}: {content}\")\n",
        "            else:\n",
        "                # No explicit speaker - try to infer\n",
        "                if content.strip():\n",
        "                    inferred_speaker = None\n",
        "\n",
        "                    # Always use the last explicit speaker for continuations\n",
        "                    if last_explicit_speaker:\n",
        "                        inferred_speaker = last_explicit_speaker\n",
        "\n",
        "                    if inferred_speaker:\n",
        "                        processed_lines.append(f\"{inferred_speaker}: {content}\")\n",
        "                        if not para_speaker:  # Update paragraph speaker if not set\n",
        "                            para_speaker = inferred_speaker\n",
        "                    else:\n",
        "                        processed_lines.append(content)\n",
        "\n",
        "        # Update current speaker based on this paragraph\n",
        "        if para_speaker:\n",
        "            current_speaker = para_speaker\n",
        "\n",
        "        if processed_lines:\n",
        "            processed_paragraphs.append('\\n'.join(processed_lines))\n",
        "\n",
        "    return processed_paragraphs\n",
        "\n",
        "def preprocess_text_for_chunking(text):\n",
        "    \"\"\"Preprocess text while preserving speaker information and handling paragraph continuations\"\"\"\n",
        "\n",
        "    # First, remove timestamps if configured\n",
        "    if not config.PRESERVE_TIMESTAMPS:\n",
        "        text = remove_timestamps(text)\n",
        "        print(\"üïí Timestamps removed from text\")\n",
        "\n",
        "    # Handle speaker preservation with improved continuation logic\n",
        "    if not config.PRESERVE_SPEAKER_LABELS:\n",
        "        return text\n",
        "\n",
        "    # Split into paragraphs\n",
        "    paragraphs = text.split('\\n\\n') if config.USE_PARAGRAPH_BOUNDARIES else [text]\n",
        "    processed_paragraphs = []\n",
        "\n",
        "    # Global speaker tracking across paragraphs\n",
        "    current_speaker = None\n",
        "    last_explicit_speaker = None\n",
        "\n",
        "    for i, para in enumerate(paragraphs):\n",
        "        if not para.strip():\n",
        "            continue\n",
        "\n",
        "        lines = para.split('\\n')\n",
        "        processed_lines = []\n",
        "        para_has_speaker = False\n",
        "\n",
        "        # First check if this paragraph starts with a speaker label\n",
        "        first_line_speaker, _ = extract_speaker_label(lines[0].strip() if lines else \"\")\n",
        "        if first_line_speaker:\n",
        "            para_has_speaker = True\n",
        "            current_speaker = first_line_speaker\n",
        "            last_explicit_speaker = first_line_speaker\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            speaker, content = extract_speaker_label(line)\n",
        "\n",
        "            if speaker:\n",
        "                # Explicit speaker found - update current speaker\n",
        "                current_speaker = speaker\n",
        "                last_explicit_speaker = speaker\n",
        "                if content.strip():  # Only add if there's actual content\n",
        "                    processed_lines.append(f\"{speaker}: {content}\")\n",
        "            else:\n",
        "                # No explicit speaker - this is a continuation\n",
        "                if content.strip():\n",
        "                    # Use the last explicit speaker for continuation\n",
        "                    if last_explicit_speaker:\n",
        "                        processed_lines.append(f\"{last_explicit_speaker}: {content}\")\n",
        "                    else:\n",
        "                        processed_lines.append(content)\n",
        "\n",
        "        if processed_lines:\n",
        "            processed_paragraphs.append('\\n'.join(processed_lines))\n",
        "\n",
        "    result = '\\n\\n'.join(processed_paragraphs)\n",
        "\n",
        "    # Count speaker continuations for user feedback\n",
        "    original_speaker_lines = len([line for line in text.split('\\n') if extract_speaker_label(line)[0] is not None])\n",
        "    processed_speaker_lines = len([line for line in result.split('\\n') if extract_speaker_label(line)[0] is not None])\n",
        "\n",
        "    if processed_speaker_lines > original_speaker_lines:\n",
        "        added_labels = processed_speaker_lines - original_speaker_lines\n",
        "        print(f\"üë• Added speaker labels to {added_labels} continuation lines\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def chunk_with_embeddings(text, similarity_threshold=None, max_chunk_size=None, min_chunk_size=None):\n",
        "    \"\"\"Chunk text using sentence embeddings (no API needed)\"\"\"\n",
        "\n",
        "    # Use config values if not provided\n",
        "    similarity_threshold = similarity_threshold or config.SIMILARITY_THRESHOLD\n",
        "    max_chunk_size = max_chunk_size or config.MAX_CHUNK_SENTENCES\n",
        "    min_chunk_size = min_chunk_size or config.MIN_CHUNK_SENTENCES\n",
        "\n",
        "    print(f\"üß† Loading model: {config.MODEL_NAME}\")\n",
        "    try:\n",
        "        model = SentenceTransformer(config.MODEL_NAME)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading model: {e}\")\n",
        "        return [text]  # Return original text as fallback\n",
        "\n",
        "    # Preprocess text (includes timestamp removal if enabled)\n",
        "    processed_text = preprocess_text_for_chunking(text)\n",
        "\n",
        "    # Split into sentences\n",
        "    try:\n",
        "        sentences = sent_tokenize(processed_text)\n",
        "    except:\n",
        "        # Fallback sentence splitting\n",
        "        sentences = re.split(r'[.!?]+\\s+', processed_text)\n",
        "        sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    print(f\"üìù Processing {len(sentences)} sentences\")\n",
        "\n",
        "    if len(sentences) <= 1:\n",
        "        return [text]\n",
        "\n",
        "    # Filter out very short sentences if configured\n",
        "    if config.MIN_SENTENCE_LENGTH > 0:\n",
        "        filtered_sentences = []\n",
        "        for sent in sentences:\n",
        "            # Clean sentence for word counting (remove speaker labels for counting)\n",
        "            _, content = extract_speaker_label(sent)\n",
        "            word_count = len(content.split())\n",
        "            if word_count >= config.MIN_SENTENCE_LENGTH:\n",
        "                filtered_sentences.append(sent)\n",
        "            elif filtered_sentences:  # Append short sentences to previous if exists\n",
        "                filtered_sentences[-1] += \" \" + sent\n",
        "        sentences = filtered_sentences\n",
        "\n",
        "    if len(sentences) <= 1:\n",
        "        return [text]\n",
        "\n",
        "    print(\"üîÑ Computing sentence embeddings...\")\n",
        "    try:\n",
        "        # Clean sentences for embedding (remove speaker labels for similarity calculation)\n",
        "        clean_sentences = []\n",
        "        for sent in sentences:\n",
        "            _, content = extract_speaker_label(sent)\n",
        "            clean_sentences.append(content.strip())\n",
        "\n",
        "        print(f\"üìä Encoding {len(clean_sentences)} sentences with {config.MODEL_NAME}...\")\n",
        "        embeddings = model.encode(clean_sentences, show_progress_bar=True)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error computing embeddings: {e}\")\n",
        "        return [text]\n",
        "\n",
        "    # Calculate similarities between consecutive sentences\n",
        "    similarities = []\n",
        "    for i in range(len(embeddings) - 1):\n",
        "        similarity = np.dot(embeddings[i], embeddings[i+1]) / (\n",
        "            np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i+1])\n",
        "        )\n",
        "        similarities.append(similarity)\n",
        "\n",
        "    print(f\"üìä Average similarity: {np.mean(similarities):.3f}\")\n",
        "\n",
        "    # Create chunks based on similarity drops\n",
        "    chunks = []\n",
        "    current_chunk = [sentences[0]]\n",
        "\n",
        "    for i, similarity in enumerate(similarities):\n",
        "        next_sentence = sentences[i + 1]\n",
        "\n",
        "        # Decide whether to start new chunk\n",
        "        start_new_chunk = False\n",
        "\n",
        "        # Check similarity threshold\n",
        "        if similarity < similarity_threshold:\n",
        "            start_new_chunk = True\n",
        "\n",
        "        # Check max chunk size\n",
        "        if len(current_chunk) >= max_chunk_size:\n",
        "            start_new_chunk = True\n",
        "\n",
        "        # Check paragraph boundaries if enabled\n",
        "        if config.USE_PARAGRAPH_BOUNDARIES:\n",
        "            if current_chunk and current_chunk[-1].endswith('\\n\\n'):\n",
        "                start_new_chunk = True\n",
        "\n",
        "        if start_new_chunk and len(current_chunk) >= min_chunk_size:\n",
        "            # Finish current chunk\n",
        "            chunk_text = ' '.join(current_chunk).replace('\\n\\n', '\\n').strip()\n",
        "            chunks.append(chunk_text)\n",
        "            current_chunk = [next_sentence]\n",
        "        else:\n",
        "            current_chunk.append(next_sentence)\n",
        "\n",
        "    # Add final chunk\n",
        "    if current_chunk:\n",
        "        if len(current_chunk) >= min_chunk_size:\n",
        "            chunk_text = ' '.join(current_chunk).replace('\\n\\n', '\\n').strip()\n",
        "            chunks.append(chunk_text)\n",
        "        elif chunks:\n",
        "            # Merge with last chunk if too small\n",
        "            chunk_text = ' '.join(current_chunk).replace('\\n\\n', '\\n').strip()\n",
        "            chunks[-1] += ' ' + chunk_text\n",
        "        else:\n",
        "            # Force include even if small (only chunk)\n",
        "            chunk_text = ' '.join(current_chunk).replace('\\n\\n', '\\n').strip()\n",
        "            chunks.append(chunk_text)\n",
        "\n",
        "\n",
        "    print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
        "    return chunks\n",
        "\n",
        "def chunk_with_claude(text, api_key=None, max_chunk_sentences=None):\n",
        "    \"\"\"Chunk text using Claude API for more intelligent segmentation\"\"\"\n",
        "\n",
        "    api_key = api_key or config.CLAUDE_API_KEY\n",
        "    max_chunk_sentences = max_chunk_sentences or config.MAX_CHUNK_SENTENCES\n",
        "\n",
        "    if not api_key:\n",
        "        print(\"‚ùå Claude API key required for Claude chunking method\")\n",
        "        return [text]\n",
        "\n",
        "    try:\n",
        "        import anthropic\n",
        "        client = anthropic.Anthropic(api_key=api_key)\n",
        "        print(f\"ü§ñ Using Claude API ({config.CLAUDE_MODEL}) for intelligent chunking...\")\n",
        "    except ImportError:\n",
        "        print(\"‚ùå anthropic package not installed. Using embeddings fallback.\")\n",
        "        return chunk_with_embeddings(text)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Claude API error: {e}\")\n",
        "        return chunk_with_embeddings(text)\n",
        "\n",
        "    # Preprocess text (includes timestamp removal if enabled)\n",
        "    processed_text = preprocess_text_for_chunking(text)\n",
        "\n",
        "    # Split into manageable sections for API\n",
        "    try:\n",
        "        sentences = sent_tokenize(processed_text)\n",
        "    except:\n",
        "        sentences = re.split(r'[.!?]+\\s+', processed_text)\n",
        "        sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    # Process in batches to stay within API limits\n",
        "    batch_size = 30  # sentences per API call\n",
        "    all_chunks = []\n",
        "\n",
        "    # Calculate total batches for progress tracking\n",
        "    total_sentences = len(sentences)\n",
        "    total_batches = (total_sentences + batch_size - 1) // batch_size\n",
        "\n",
        "    print(f\"üìä Processing {total_sentences} sentences in {total_batches} batches...\")\n",
        "    print(f\"üîÑ Progress: [\", end='', flush=True)\n",
        "\n",
        "    for batch_num, i in enumerate(range(0, len(sentences), batch_size)):\n",
        "        batch_sentences = sentences[i:i + batch_size]\n",
        "        batch_text = ' '.join(batch_sentences)\n",
        "\n",
        "        # Update progress bar\n",
        "        progress = int((batch_num / total_batches) * 20)\n",
        "        print(\"=\" * (progress - (batch_num - 1) * 20 // total_batches), end='', flush=True)\n",
        "\n",
        "        # Skip very short batches\n",
        "        if len(batch_text.strip()) < 50:\n",
        "            if all_chunks:\n",
        "                all_chunks[-1] += \" \" + batch_text\n",
        "            else:\n",
        "                all_chunks.append(batch_text)\n",
        "            continue\n",
        "\n",
        "        prompt = f\"\"\"Please analyze this interview transcript and break it into semantically coherent chunks. Each chunk should:\n",
        "\n",
        "1. Contain at most {max_chunk_sentences} sentences\n",
        "2. Represent a distinct topic, question-answer pair, or conversation turn\n",
        "3. Preserve speaker labels exactly as they appear (Q:, A:, Interviewer:, etc.)\n",
        "4. Maintain natural conversation flow\n",
        "\n",
        "Return ONLY the chunks, each separated by exactly \"---CHUNK_BREAK---\"\n",
        "\n",
        "Text to chunk:\n",
        "{batch_text}\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = client.messages.create(\n",
        "                model=config.CLAUDE_MODEL,\n",
        "                max_tokens=4000,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            )\n",
        "\n",
        "            # Parse response\n",
        "            response_text = response.content[0].text if response.content else \"\"\n",
        "            batch_chunks = response_text.split(\"---CHUNK_BREAK---\")\n",
        "            batch_chunks = [chunk.strip() for chunk in batch_chunks if chunk.strip()]\n",
        "\n",
        "            if not batch_chunks:\n",
        "                # Fallback if Claude didn't return proper format\n",
        "                print(f\"\\n‚ö†Ô∏è Claude response format issue, using fallback for batch {batch_num + 1}/{total_batches}\")\n",
        "                print(f\"üîÑ Progress: [{'=' * progress}\", end='', flush=True)\n",
        "                fallback_chunks = []\n",
        "                for j in range(0, len(batch_sentences), max_chunk_sentences):\n",
        "                    chunk_sentences = batch_sentences[j:j + max_chunk_sentences]\n",
        "                    fallback_chunks.append(' '.join(chunk_sentences))\n",
        "                all_chunks.extend(fallback_chunks)\n",
        "            else:\n",
        "                all_chunks.extend(batch_chunks)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ö†Ô∏è API error for batch {batch_num + 1}/{total_batches}: {e}\")\n",
        "            print(f\"üîÑ Progress: [{'=' * progress}\", end='', flush=True)\n",
        "            # Fallback to simple splitting for this batch\n",
        "            fallback_chunks = []\n",
        "            for j in range(0, len(batch_sentences), max_chunk_sentences):\n",
        "                chunk_sentences = batch_sentences[j:j + max_chunk_sentences]\n",
        "                fallback_chunks.append(' '.join(chunk_sentences))\n",
        "            all_chunks.extend(fallback_chunks)\n",
        "\n",
        "    print(\"=\" * (20 - progress) + \"] 100%\")\n",
        "    print(f\"‚úÖ Created {len(all_chunks)} chunks using Claude API\")\n",
        "    return all_chunks\n",
        "\n",
        "def analyze_chunks_quality(original_text, chunks):\n",
        "    \"\"\"Analyze the quality of chunking results\"\"\"\n",
        "\n",
        "    # Basic preservation check\n",
        "    original_words = len(original_text.split())\n",
        "    chunk_words = sum(len(chunk.split()) for chunk in chunks)\n",
        "    preservation_rate = (chunk_words / original_words) * 100 if original_words > 0 else 0\n",
        "\n",
        "    # Chunk statistics\n",
        "    chunk_lengths = [len(chunk.split()) for chunk in chunks]\n",
        "\n",
        "    stats = {\n",
        "        'total_chunks': len(chunks),\n",
        "        'preservation_rate': preservation_rate,\n",
        "        'avg_words_per_chunk': np.mean(chunk_lengths),\n",
        "        'min_words': min(chunk_lengths) if chunk_lengths else 0,\n",
        "        'max_words': max(chunk_lengths) if chunk_lengths else 0,\n",
        "        'std_words': np.std(chunk_lengths) if len(chunk_lengths) > 1 else 0\n",
        "    }\n",
        "\n",
        "    return stats\n",
        "\n",
        "print(\"‚úÖ Core processing functions loaded successfully!\")\n",
        "print(\"üîß Features: Semantic chunking, speaker preservation, timestamp removal\")"
      ],
      "metadata": {
        "id": "HjVaPaCaAbpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Processing Pipeline and Output\n",
        "\n",
        "Execute the complete chunking workflow with your uploaded files and configured parameters. This orchestrates all previous functions and provides real-time progress feedback and quality validation."
      ],
      "metadata": {
        "id": "XuqAQBcJAlG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Processing Pipeline\n",
        "\n",
        "def check_prerequisites(uploaded_files):\n",
        "    \"\"\"Check if everything is ready for processing\"\"\"\n",
        "    issues = []\n",
        "\n",
        "    # Check if files are uploaded\n",
        "    if not uploaded_files:\n",
        "        issues.append(\"‚ùå No files uploaded\")\n",
        "\n",
        "    # Check configuration\n",
        "    if config.CHUNKING_METHOD == 'claude' and not config.CLAUDE_API_KEY:\n",
        "        issues.append(\"‚ùå Claude API key required for Claude method\")\n",
        "\n",
        "    if issues:\n",
        "        for issue in issues:\n",
        "            print(issue)\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# Removed select_file_for_processing as it used input()\n",
        "\n",
        "def run_chunking_process(filename, text):\n",
        "    \"\"\"Run the main chunking process\"\"\"\n",
        "\n",
        "    print(\"\\\\n\" + \"=\" * 60)\n",
        "    print(\"üöÄ CHUNKING PROCESS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"üìÑ Processing: {filename}\")\n",
        "    print(f\"üìä Text length: {len(text):,} characters, {len(text.split()):,} words\")\n",
        "    print(f\"üß† Method: {config.CHUNKING_METHOD.title()}\")\n",
        "    print(f\"‚öôÔ∏è Settings: {config.MIN_CHUNK_SENTENCES}-{config.MAX_CHUNK_SENTENCES} sentences, threshold: {config.SIMILARITY_THRESHOLD}\")\n",
        "    print()\n",
        "\n",
        "    # Run chunking based on method\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    try:\n",
        "        if config.CHUNKING_METHOD == 'claude':\n",
        "            chunks = chunk_with_claude(text)\n",
        "        else:\n",
        "            chunks = chunk_with_embeddings(text)\n",
        "\n",
        "        end_time = datetime.now()\n",
        "        processing_time = (end_time - start_time).total_seconds()\n",
        "\n",
        "        print(f\"\\\\n‚è±Ô∏è Processing completed in {processing_time:.1f} seconds\")\n",
        "\n",
        "        if not chunks:\n",
        "            print(\"‚ùå No chunks created. Please check your settings.\")\n",
        "            return None\n",
        "\n",
        "        # Analyze quality\n",
        "        quality_stats = analyze_chunks_quality(text, chunks)\n",
        "\n",
        "        print(f\"\\\\nüìä Chunking Results:\")\n",
        "        print(f\"   ‚Ä¢ Total chunks: {quality_stats['total_chunks']}\")\n",
        "        print(f\"   ‚Ä¢ Text preservation: {quality_stats['preservation_rate']:.1f}%\")\n",
        "        print(f\"   ‚Ä¢ Average words per chunk: {quality_stats['avg_words_per_chunk']:.1f}\")\n",
        "        print(f\"   ‚Ä¢ Chunk size range: {quality_stats['min_words']}-{quality_stats['max_words']} words\")\n",
        "\n",
        "        if quality_stats['preservation_rate'] < 95:\n",
        "            print(\"   ‚ö†Ô∏è Warning: Significant text loss detected\")\n",
        "        else:\n",
        "            print(\"   ‚úÖ Good text preservation\")\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Chunking failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_dataframe_from_chunks(chunks, filename):\n",
        "    \"\"\"Create a comprehensive DataFrame from chunks\"\"\"\n",
        "\n",
        "    chunk_data = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        # Basic metrics\n",
        "        word_count = len(chunk.split())\n",
        "        char_count = len(chunk)\n",
        "        sentence_count = len([s for s in re.split(r'[.!?]+', chunk) if s.strip()])\n",
        "\n",
        "        data = {\n",
        "            'chunk_id': i + 1,\n",
        "            'text': chunk,\n",
        "            'word_count': word_count,\n",
        "            'char_count': char_count,\n",
        "            'sentence_count': sentence_count\n",
        "        }\n",
        "\n",
        "        # Add speaker information if enabled\n",
        "        if config.INCLUDE_SPEAKER_COLUMN:\n",
        "            # Extract speaker from the chunk - look for any speaker label in the chunk\n",
        "            speaker = None\n",
        "            lines = chunk.split('\\n')\n",
        "            for line in lines:\n",
        "                line_speaker, _ = extract_speaker_label(line.strip())\n",
        "                if line_speaker:\n",
        "                    speaker = line_speaker\n",
        "                    break\n",
        "\n",
        "            # If no speaker found in any line, mark as Unknown\n",
        "            data['speaker'] = speaker if speaker else 'Unknown'\n",
        "\n",
        "        # Add metadata if enabled\n",
        "        if config.INCLUDE_METADATA:\n",
        "            data['avg_words_per_sentence'] = word_count / max(1, sentence_count)\n",
        "            data['source_file'] = filename\n",
        "\n",
        "        # Add timestamp if enabled\n",
        "        if config.TIMESTAMP_OUTPUT:\n",
        "            data['processed_timestamp'] = datetime.now().isoformat()\n",
        "\n",
        "        chunk_data.append(data)\n",
        "\n",
        "    return pd.DataFrame(chunk_data)\n",
        "\n",
        "def create_processing_interface():\n",
        "    \"\"\"Create the main processing interface\"\"\"\n",
        "\n",
        "    # Instructions\n",
        "    instructions_html = \"\"\"\n",
        "    <div style='background-color: #E7ECEF; padding: 20px; border-radius: 10px; margin: 20px 0; border-left: 5px solid #274C77;'>\n",
        "    <h3 style='color: #274C77; margin-top: 0;'>üöÄ Process Your Interview Transcript</h3>\n",
        "    <p><strong>Everything is ready!</strong> Click the button below to run the semantic chunking process on your uploaded files.</p>\n",
        "    <div style='background-color: #A3CEF1; padding: 15px; border-radius: 8px; margin: 15px 0; border-left: 4px solid #6096BA;'>\n",
        "        <p style='color: #274C77; margin: 0; font-weight: bold;'>What happens next:</p>\n",
        "        <ol style='color: #274C77; margin: 10px 0;'>\n",
        "            <li>üîç Text preprocessing and speaker label preservation</li>\n",
        "            <li>üß† Semantic analysis using your chosen method</li>\n",
        "            <li>‚úÇÔ∏è Intelligent chunking based on content similarity</li>\n",
        "            <li>üìä Quality analysis and statistics</li>\n",
        "            <li>üíæ Export preparation in your chosen format</li>\n",
        "        </ol>\n",
        "    </div>\n",
        "    <p><strong>üí° Tip:</strong> The process may take a few minutes for large files. Watch the progress output below!</p>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    # Process button\n",
        "    process_button = widgets.Button(\n",
        "        description='üöÄ Start Chunking Process',\n",
        "        disabled=False,\n",
        "        button_style='',\n",
        "        tooltip='Begin processing your uploaded transcript',\n",
        "        icon='play',\n",
        "        layout=widgets.Layout(width='300px', height='50px'),\n",
        "        style={'button_color': '#6096BA', 'font_weight': 'bold'}\n",
        "    )\n",
        "\n",
        "    # Reset button\n",
        "    reset_button = widgets.Button(\n",
        "        description='üîÑ Reset All',\n",
        "        disabled=False,\n",
        "        button_style='',\n",
        "        tooltip='Clear everything and start over',\n",
        "        icon='refresh',\n",
        "        layout=widgets.Layout(width='150px', height='40px'),\n",
        "        style={'button_color': '#8B8C89', 'font_weight': 'bold'}\n",
        "    )\n",
        "\n",
        "    # Status display\n",
        "    status_output = widgets.Output()\n",
        "\n",
        "    # Global variable to store results\n",
        "    global processing_results\n",
        "    processing_results = {}\n",
        "\n",
        "    def run_full_process(b):\n",
        "        with status_output:\n",
        "            status_output.clear_output()\n",
        "\n",
        "            # Access the global uploaded_files from the upload interface\n",
        "            global uploaded_files\n",
        "\n",
        "            # Check prerequisites\n",
        "            if not check_prerequisites(uploaded_files):\n",
        "                print(\"\\\\nüí° Please complete the steps above before processing:\")\n",
        "                print(\"   1. Configure your settings\")\n",
        "                print(\"   2. Upload your transcript files\")\n",
        "                return\n",
        "\n",
        "            try:\n",
        "                # Select file or combine files based on uploaded_files\n",
        "                if len(uploaded_files) == 1:\n",
        "                    filename = list(uploaded_files.keys())[0]\n",
        "                    text = uploaded_files[filename]\n",
        "                    print(f\"üìÑ Processing single file: {filename}\")\n",
        "                elif len(uploaded_files) > 1:\n",
        "                     # Combine all files\n",
        "                    combined_text = []\n",
        "                    combined_filename = f\"combined_{len(uploaded_files)}_files\"\n",
        "\n",
        "                    for i, (fname, ftext) in enumerate(uploaded_files.items(), 1):\n",
        "                        combined_text.append(f\"=== Document {i}: {fname} ===\")\n",
        "                        combined_text.append(ftext)\n",
        "                        combined_text.append(\"\")  # Blank line between files\n",
        "\n",
        "                    text = '\\\\n\\\\n'.join(combined_text)\n",
        "                    filename = combined_filename\n",
        "                    print(f\"üîó Combined {len(uploaded_files)} files ({len(text.split()):,} total words) for processing\")\n",
        "                else:\n",
        "                    print(\"‚ùå No files available for processing.\")\n",
        "                    return\n",
        "\n",
        "\n",
        "                # Run chunking\n",
        "                chunks = run_chunking_process(filename, text)\n",
        "\n",
        "                if not chunks:\n",
        "                    return\n",
        "\n",
        "                # Create DataFrame\n",
        "                print(\"\\\\nüìã Creating structured output...\")\n",
        "                df = create_dataframe_from_chunks(chunks, filename)\n",
        "\n",
        "                # Store results globally\n",
        "                global processing_results\n",
        "                processing_results = {\n",
        "                    'filename': filename,\n",
        "                    'original_text': text,\n",
        "                    'chunks': chunks,\n",
        "                    'dataframe': df,\n",
        "                    'timestamp': datetime.now()\n",
        "                }\n",
        "\n",
        "                # Display preview\n",
        "                print(\"\\\\n\" + \"=\" * 60)\n",
        "                print(\"üìã PREVIEW OF RESULTS\")\n",
        "                print(\"=\" * 60)\n",
        "\n",
        "                # Show first few chunks\n",
        "                display_df = df.head(3)\n",
        "                if 'text' in display_df.columns:\n",
        "                    # Truncate text for display\n",
        "                    display_df = display_df.copy()\n",
        "                    display_df['text_preview'] = display_df['text'].str[:100] + '...'\n",
        "                    display_df = display_df.drop('text', axis=1)\n",
        "\n",
        "                display(display_df)\n",
        "\n",
        "                print(f\"\\\\n‚úÖ Processing complete! {len(chunks)} chunks created.\")\n",
        "                print(\"üëá Continue to the next step to export your results.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Processing failed: {e}\")\n",
        "                import traceback\n",
        "                print(\"\\\\nDetailed error:\")\n",
        "                traceback.print_exc()\n",
        "\n",
        "    def reset_everything(b):\n",
        "        with status_output:\n",
        "            status_output.clear_output()\n",
        "\n",
        "            global uploaded_files, file_stats, processing_results\n",
        "            uploaded_files.clear()\n",
        "            file_stats.clear()\n",
        "            processing_results.clear()\n",
        "\n",
        "            # Reset file info display in the upload interface\n",
        "            upload_interface['file_info'].value = \"<p style='color: #666; font-style: italic;'>Files cleared. Click 'Choose Files to Upload' to start over.</p>\"\n",
        "\n",
        "\n",
        "            print(\"üîÑ Everything has been reset.\")\n",
        "            print(\"üëÜ You can now reconfigure and upload new files.\")\n",
        "\n",
        "    # Bind events\n",
        "    process_button.on_click(run_full_process)\n",
        "    reset_button.on_click(reset_everything)\n",
        "\n",
        "    # Layout\n",
        "    buttons_container = widgets.HBox([process_button, reset_button])\n",
        "\n",
        "    # Display interface\n",
        "    display(HTML(instructions_html))\n",
        "    display(buttons_container)\n",
        "    display(status_output)\n",
        "\n",
        "    return {\n",
        "        'process_button': process_button,\n",
        "        'reset_button': reset_button,\n",
        "        'output': status_output\n",
        "    }\n",
        "\n",
        "# Initialize processing interface\n",
        "print(\"üöÄ Main Processing Pipeline Ready\")\n",
        "print(\"üëÜ Make sure you've configured settings and uploaded files above!\")\n",
        "\n",
        "# Global variable for results\n",
        "processing_results = {}\n",
        "\n",
        "# Create and display processing interface\n",
        "processing_interface = create_processing_interface()"
      ],
      "metadata": {
        "id": "ZWRbPd98AoDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results Analsys & Export\n",
        "\n",
        "Analyze chunking quality with detailed statistics and visualizations, then export results in your preferred format. Generate reports and download structured data ready for qualitative analysis software."
      ],
      "metadata": {
        "id": "nl_2yCA5C5V7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Results Analysis and Export\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "def generate_comprehensive_statistics(df, original_text):\n",
        "    \"\"\"Generate detailed statistics about the chunking results\"\"\"\n",
        "\n",
        "    if df is None or len(df) == 0:\n",
        "        print(\"‚ùå No data available for analysis\")\n",
        "        return\n",
        "\n",
        "    print(\"\\\\n\" + \"=\" * 70)\n",
        "    print(\"üìä COMPREHENSIVE CHUNKING ANALYSIS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Basic metrics\n",
        "    total_chunks = len(df)\n",
        "    total_words = df['word_count'].sum()\n",
        "    avg_words = df['word_count'].mean()\n",
        "    median_words = df['word_count'].median()\n",
        "    std_words = df['word_count'].std()\n",
        "\n",
        "    print(f\"\\\\nüìã Basic Metrics:\")\n",
        "    print(f\"   ‚Ä¢ Total chunks created: {total_chunks}\")\n",
        "    print(f\"   ‚Ä¢ Total words processed: {total_words:,}\")\n",
        "    print(f\"   ‚Ä¢ Average words per chunk: {avg_words:.1f}\")\n",
        "    print(f\"   ‚Ä¢ Median words per chunk: {median_words:.1f}\")\n",
        "    print(f\"   ‚Ä¢ Standard deviation: {std_words:.1f}\")\n",
        "    print(f\"   ‚Ä¢ Shortest chunk: {df['word_count'].min()} words\")\n",
        "    print(f\"   ‚Ä¢ Longest chunk: {df['word_count'].max()} words\")\n",
        "\n",
        "    # Size distribution analysis\n",
        "    word_ranges = {\n",
        "        'Very Short (1-15 words)': len(df[df['word_count'] <= 15]),\n",
        "        'Short (16-30 words)': len(df[(df['word_count'] > 15) & (df['word_count'] <= 30)]),\n",
        "        'Medium (31-60 words)': len(df[(df['word_count'] > 30) & (df['word_count'] <= 60)]),\n",
        "        'Long (61-100 words)': len(df[(df['word_count'] > 60) & (df['word_count'] <= 100)]),\n",
        "        'Very Long (100+ words)': len(df[df['word_count'] > 100])\n",
        "    }\n",
        "\n",
        "    print(f\"\\\\nüìè Chunk Size Distribution:\")\n",
        "    for range_name, count in word_ranges.items():\n",
        "        percentage = (count / total_chunks) * 100\n",
        "        bar = \"‚ñà\" * int(percentage / 5)  # Simple text bar\n",
        "        print(f\"   ‚Ä¢ {range_name:25} {count:3d} ({percentage:5.1f}%) {bar}\")\n",
        "\n",
        "    # Speaker analysis if available\n",
        "    if 'speaker' in df.columns:\n",
        "        unique_speakers = df['speaker'].nunique()\n",
        "        print(f\"\\\\nüë• Speaker Analysis:\")\n",
        "        print(f\"   ‚Ä¢ Unique speakers identified: {unique_speakers}\")\n",
        "\n",
        "        speaker_counts = df['speaker'].value_counts()\n",
        "        speaker_word_totals = df.groupby('speaker')['word_count'].sum().sort_values(ascending=False)\n",
        "\n",
        "        print(f\"   ‚Ä¢ Most active speakers (by chunks):\")\n",
        "        for i, (speaker, count) in enumerate(speaker_counts.head(5).items()):\n",
        "            percentage = (count / total_chunks) * 100\n",
        "            word_total = speaker_word_totals.get(speaker, 0)\n",
        "            print(f\"     {i+1}. {speaker}: {count} chunks ({percentage:.1f}%), {word_total:,} words\")\n",
        "\n",
        "    # Text preservation analysis\n",
        "    original_words = len(original_text.split())\n",
        "    preservation_rate = (total_words / original_words) * 100 if original_words > 0 else 0\n",
        "\n",
        "    print(f\"\\\\nüîç Text Preservation Analysis:\")\n",
        "    print(f\"   ‚Ä¢ Original text: {original_words:,} words\")\n",
        "    print(f\"   ‚Ä¢ Processed text: {total_words:,} words\")\n",
        "    print(f\"   ‚Ä¢ Preservation rate: {preservation_rate:.2f}%\")\n",
        "\n",
        "    if preservation_rate >= 99:\n",
        "        print(\"   ‚úÖ Excellent preservation - minimal text loss\")\n",
        "    elif preservation_rate >= 95:\n",
        "        print(\"   ‚úÖ Good preservation - acceptable for analysis\")\n",
        "    elif preservation_rate >= 90:\n",
        "        print(\"   ‚ö†Ô∏è Moderate preservation - review recommended\")\n",
        "    else:\n",
        "        print(\"   ‚ùå Poor preservation - significant text loss detected\")\n",
        "\n",
        "    # Quality recommendations\n",
        "    print(f\"\\\\nüí° Quality Assessment for Qualitative Analysis:\")\n",
        "\n",
        "    if avg_words < 20:\n",
        "        print(\"   üìù Small chunks - Good for detailed coding, but may lack context\")\n",
        "    elif avg_words < 50:\n",
        "        print(\"   üìù Medium chunks - Balanced for most qualitative analysis methods\")\n",
        "    else:\n",
        "        print(\"   üìù Large chunks - Good for thematic analysis, may need sub-coding\")\n",
        "\n",
        "    if std_words > avg_words * 0.5:\n",
        "        print(\"   üìä High variability in chunk sizes - consider adjusting settings\")\n",
        "    else:\n",
        "        print(\"   üìä Consistent chunk sizes - good for systematic analysis\")\n",
        "\n",
        "def create_visualizations(df):\n",
        "    \"\"\"Create helpful visualizations of the chunking results\"\"\"\n",
        "\n",
        "    if df is None or len(df) == 0:\n",
        "        print(\"‚ùå No data available for visualization\")\n",
        "        return\n",
        "\n",
        "    print(\"\\\\nüìà Generating visualizations...\")\n",
        "\n",
        "    # Set up the plot style\n",
        "    plt.style.use('default')\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('Interview Transcript Chunking Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # 1. Histogram of chunk word counts\n",
        "    ax1.hist(df['word_count'], bins=min(20, len(df)//2 + 1), edgecolor='black', alpha=0.7, color='skyblue')\n",
        "    ax1.set_xlabel('Words per Chunk')\n",
        "    ax1.set_ylabel('Number of Chunks')\n",
        "    ax1.set_title('Distribution of Chunk Sizes')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.axvline(df['word_count'].mean(), color='red', linestyle='--', alpha=0.7, label=f'Mean: {df[\"word_count\"].mean():.1f}')\n",
        "    ax1.legend()\n",
        "\n",
        "    # 2. Line plot showing chunk sizes throughout document\n",
        "    ax2.plot(df['chunk_id'], df['word_count'], marker='o', linewidth=2, markersize=3, color='coral', alpha=0.8)\n",
        "    ax2.set_xlabel('Chunk Number')\n",
        "    ax2.set_ylabel('Word Count')\n",
        "    ax2.set_title('Chunk Sizes Throughout Document')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.axhline(df['word_count'].mean(), color='green', linestyle='--', alpha=0.7, label=f'Average: {df[\"word_count\"].mean():.1f}')\n",
        "    ax2.legend()\n",
        "\n",
        "    # 3. Box plot of word counts by speaker (if available) or general distribution\n",
        "    if 'speaker' in df.columns and df['speaker'].nunique() > 1:\n",
        "        # Filter to top speakers for readability\n",
        "        top_speakers = df['speaker'].value_counts().head(6).index\n",
        "        filtered_df = df[df['speaker'].isin(top_speakers)]\n",
        "\n",
        "        speaker_data = [filtered_df[filtered_df['speaker'] == speaker]['word_count'].values\n",
        "                       for speaker in top_speakers]\n",
        "\n",
        "        bp = ax3.boxplot(speaker_data, labels=top_speakers, patch_artist=True)\n",
        "        for patch in bp['boxes']:\n",
        "            patch.set_facecolor('lightgreen')\n",
        "            patch.set_alpha(0.7)\n",
        "\n",
        "        ax3.set_ylabel('Word Count')\n",
        "        ax3.set_title('Chunk Size Distribution by Speaker')\n",
        "        ax3.tick_params(axis='x', rotation=45)\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "    else:\n",
        "        # General box plot\n",
        "        bp = ax3.boxplot(df['word_count'], patch_artist=True)\n",
        "        bp['boxes'][0].set_facecolor('lightgreen')\n",
        "        bp['boxes'][0].set_alpha(0.7)\n",
        "        ax3.set_ylabel('Word Count')\n",
        "        ax3.set_title('Overall Chunk Size Distribution')\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Speaker distribution pie chart (if available) or sentence count analysis\n",
        "    if 'speaker' in df.columns and df['speaker'].nunique() > 1:\n",
        "        speaker_counts = df['speaker'].value_counts().head(8)  # Top 8 speakers\n",
        "        colors = plt.cm.Set3(np.linspace(0, 1, len(speaker_counts)))\n",
        "\n",
        "        wedges, texts, autotexts = ax4.pie(speaker_counts.values, labels=speaker_counts.index,\n",
        "                                          autopct='%1.1f%%', colors=colors, startangle=90)\n",
        "        ax4.set_title('Distribution of Chunks by Speaker')\n",
        "\n",
        "        # Make text more readable\n",
        "        for autotext in autotexts:\n",
        "            autotext.set_color('white')\n",
        "            autotext.set_fontweight('bold')\n",
        "    else:\n",
        "        # Scatter plot: word count vs sentence count\n",
        "        if 'sentence_count' in df.columns:\n",
        "            ax4.scatter(df['word_count'], df['sentence_count'], alpha=0.6, color='purple', s=30)\n",
        "            ax4.set_xlabel('Word Count')\n",
        "            ax4.set_ylabel('Sentence Count')\n",
        "            ax4.set_title('Words vs Sentences per Chunk')\n",
        "            ax4.grid(True, alpha=0.3)\n",
        "\n",
        "            # Add trend line\n",
        "            z = np.polyfit(df['word_count'], df['sentence_count'], 1)\n",
        "            p = np.poly1d(z)\n",
        "            ax4.plot(df['word_count'], p(df['word_count']), \"r--\", alpha=0.7)\n",
        "        else:\n",
        "            ax4.text(0.5, 0.5, 'No additional\\\\nanalysis available',\n",
        "                    horizontalalignment='center', verticalalignment='center',\n",
        "                    transform=ax4.transAxes, fontsize=12)\n",
        "            ax4.set_title('Additional Analysis')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"‚úÖ Visualizations complete!\")\n",
        "\n",
        "def save_results(df, filename, original_text):\n",
        "    \"\"\"Save results in the configured format with metadata to a folder\"\"\"\n",
        "\n",
        "    if df is None or len(df) == 0:\n",
        "        print(\"‚ùå No data to save\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üíæ SAVING RESULTS TO A FOLDER\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Create output directory\n",
        "    base_name = os.path.splitext(filename)[0] if filename else \"transcript\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_dir = f\"chunked_results_{base_name}_{timestamp}\"\n",
        "\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f\"üìÅ Created directory: {output_dir}\")\n",
        "\n",
        "    try:\n",
        "        if config.OUTPUT_FORMAT == 'csv':\n",
        "            output_filename = f\"{output_dir}/{base_name}_chunked.csv\"\n",
        "            df.to_csv(output_filename, index=False, encoding='utf-8')\n",
        "\n",
        "            print(f\"\\nüìÑ CSV Format Saved:\")\n",
        "            print(f\"   File: {output_filename}\")\n",
        "            print(f\"   Contains: Simple spreadsheet with columns for chunk_id, text, word_count, etc.\")\n",
        "            print(f\"   Best for: Quick analysis in Excel/Google Sheets, simple data processing\")\n",
        "\n",
        "        elif config.OUTPUT_FORMAT == 'excel':\n",
        "            output_filename = f\"{output_dir}/{base_name}_chunked.xlsx\"\n",
        "\n",
        "            # Create Excel with multiple sheets and formatting\n",
        "            with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:\n",
        "                # Main chunks sheet\n",
        "                df.to_excel(writer, sheet_name='Chunks', index=False)\n",
        "\n",
        "                # Statistics sheet\n",
        "                stats_data = {\n",
        "                    'Metric': ['Total Chunks', 'Total Words', 'Average Words/Chunk',\n",
        "                              'Min Words', 'Max Words', 'Standard Deviation'],\n",
        "                    'Value': [len(df), df['word_count'].sum(), df['word_count'].mean(),\n",
        "                             df['word_count'].min(), df['word_count'].max(), df['word_count'].std()]\n",
        "                }\n",
        "                pd.DataFrame(stats_data).to_excel(writer, sheet_name='Statistics', index=False)\n",
        "\n",
        "                # Configuration sheet\n",
        "                config_data = {\n",
        "                    'Setting': ['Chunking Method', 'Similarity Threshold', 'Max Sentences',\n",
        "                               'Min Sentences', 'Model Name', 'Preserve Speakers'],\n",
        "                    'Value': [config.CHUNKING_METHOD, config.SIMILARITY_THRESHOLD,\n",
        "                             config.MAX_CHUNK_SENTENCES, config.MIN_CHUNK_SENTENCES,\n",
        "                             config.MODEL_NAME, config.PRESERVE_SPEAKER_LABELS]\n",
        "                }\n",
        "                pd.DataFrame(config_data).to_excel(writer, sheet_name='Configuration', index=False)\n",
        "\n",
        "                # Format the main sheet\n",
        "                workbook = writer.book\n",
        "                worksheet = writer.sheets['Chunks']\n",
        "\n",
        "                # Auto-adjust column widths\n",
        "                for column in worksheet.columns:\n",
        "                    max_length = 0\n",
        "                    column_letter = column[0].column_letter\n",
        "\n",
        "                    for cell in column:\n",
        "                        try:\n",
        "                            if len(str(cell.value)) > max_length:\n",
        "                                max_length = len(str(cell.value))\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "                    adjusted_width = min(max_length + 2, 60)  # Cap at 60\n",
        "                    worksheet.column_dimensions[column_letter].width = adjusted_width\n",
        "\n",
        "            print(f\"\\nüìä Excel Format Saved:\")\n",
        "            print(f\"   File: {output_filename}\")\n",
        "            print(f\"   Contains: 3 sheets - Chunks (main data), Statistics (summary), Configuration (settings)\")\n",
        "            print(f\"   Best for: Professional analysis, importing to NVivo/ATLAS.ti, comprehensive reporting\")\n",
        "\n",
        "        elif config.OUTPUT_FORMAT == 'json':\n",
        "            output_filename = f\"{output_dir}/{base_name}_chunked.json\"\n",
        "\n",
        "            # Create comprehensive JSON output\n",
        "            json_output = {\n",
        "                'metadata': {\n",
        "                    'source_file': filename,\n",
        "                    'processing_timestamp': datetime.now().isoformat(),\n",
        "                    'configuration': {\n",
        "                        'chunking_method': config.CHUNKING_METHOD,\n",
        "                        'similarity_threshold': config.SIMILARITY_THRESHOLD,\n",
        "                        'max_sentences': config.MAX_CHUNK_SENTENCES,\n",
        "                        'min_sentences': config.MIN_CHUNK_SENTENCES,\n",
        "                        'model_name': config.MODEL_NAME,\n",
        "                        'preserve_speakers': config.PRESERVE_SPEAKER_LABELS\n",
        "                    },\n",
        "                    'statistics': {\n",
        "                        'total_chunks': len(df),\n",
        "                        'total_words': int(df['word_count'].sum()),\n",
        "                        'average_words_per_chunk': float(df['word_count'].mean()),\n",
        "                        'min_words': int(df['word_count'].min()),\n",
        "                        'max_words': int(df['word_count'].max())\n",
        "                    }\n",
        "                },\n",
        "                'chunks': df.to_dict('records')\n",
        "            }\n",
        "\n",
        "            with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "                json.dump(json_output, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            print(f\"\\nüîß JSON Format Saved:\")\n",
        "            print(f\"   File: {output_filename}\")\n",
        "            print(f\"   Contains: Structured data with metadata, configuration, statistics, and chunks\")\n",
        "            print(f\"   Best for: Programming analysis, API integration, custom processing scripts\")\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported output format: {config.OUTPUT_FORMAT}\")\n",
        "\n",
        "        # Also save configuration for reproducibility\n",
        "        config_filename = f\"{output_dir}/processing_config.json\"\n",
        "        config_dict = {\n",
        "            'processing_info': {\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'source_file': filename,\n",
        "                'total_chunks': len(df),\n",
        "                'notebook_version': '2.0.0'\n",
        "            },\n",
        "            'chunking_parameters': {\n",
        "                'method': config.CHUNKING_METHOD,\n",
        "                'similarity_threshold': config.SIMILARITY_THRESHOLD,\n",
        "                'max_chunk_sentences': config.MAX_CHUNK_SENTENCES,\n",
        "                'min_chunk_sentences': config.MIN_CHUNK_SENTENCES,\n",
        "                'model_name': config.MODEL_NAME\n",
        "            },\n",
        "            'processing_options': {\n",
        "                'preserve_speaker_labels': config.PRESERVE_SPEAKER_LABELS,\n",
        "                'preserve_timestamps': config.PRESERVE_TIMESTAMPS,\n",
        "                'use_paragraph_boundaries': config.USE_PARAGRAPH_BOUNDARIES\n",
        "            },\n",
        "            'output_settings': {\n",
        "                'output_format': config.OUTPUT_FORMAT,\n",
        "                'include_speaker_column': config.INCLUDE_SPEAKER_COLUMN,\n",
        "                'include_metadata': config.INCLUDE_METADATA\n",
        "            }\n",
        "        }\n",
        "\n",
        "        with open(config_filename, 'w') as f:\n",
        "            json.dump(config_dict, f, indent=2)\n",
        "\n",
        "        print(f\"\\n‚öôÔ∏è Configuration saved: {config_filename}\")\n",
        "        print(f\"   Contains: All settings used for this processing run (for reproducibility)\")\n",
        "\n",
        "        # Create a summary file\n",
        "        summary_filename = f\"{output_dir}/README.txt\"\n",
        "        with open(summary_filename, 'w') as f:\n",
        "            f.write(\"INTERVIEW TRANSCRIPT CHUNKING RESULTS\\n\")\n",
        "            f.write(\"=\" * 40 + \"\\n\\n\")\n",
        "            f.write(f\"Processing Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "            f.write(f\"Source File: {filename}\\n\")\n",
        "            f.write(f\"Total Chunks Created: {len(df)}\\n\")\n",
        "            f.write(f\"Output Format: {config.OUTPUT_FORMAT.upper()}\\n\\n\")\n",
        "\n",
        "            f.write(\"FILES IN THIS DIRECTORY:\\n\")\n",
        "            f.write(f\"‚Ä¢ {os.path.basename(output_filename)} - Main chunked data\\n\")\n",
        "            f.write(f\"‚Ä¢ processing_config.json - Settings used for processing\\n\")\n",
        "            f.write(f\"‚Ä¢ README.txt - This summary file\\n\\n\")\n",
        "\n",
        "            f.write(\"EXPORT FORMAT GUIDE:\\n\")\n",
        "            if config.OUTPUT_FORMAT == 'csv':\n",
        "                f.write(\"‚Ä¢ CSV: Simple spreadsheet format, easy to open in Excel/Google Sheets\\n\")\n",
        "            elif config.OUTPUT_FORMAT == 'excel':\n",
        "                f.write(\"‚Ä¢ Excel: Multi-sheet file with data, statistics, and configuration\\n\")\n",
        "                f.write(\"  - Sheet 1 (Chunks): Your chunked interview data\\n\")\n",
        "                f.write(\"  - Sheet 2 (Statistics): Summary statistics\\n\")\n",
        "                f.write(\"  - Sheet 3 (Configuration): Processing settings\\n\")\n",
        "            elif config.OUTPUT_FORMAT == 'json':\n",
        "                f.write(\"‚Ä¢ JSON: Structured data format for programming/API use\\n\")\n",
        "\n",
        "            f.write(\"\\nNEXT STEPS:\\n\")\n",
        "            f.write(\"‚Ä¢ Import the main data file into your qualitative analysis software\\n\")\n",
        "            f.write(\"‚Ä¢ Use the configuration file to reproduce these results\\n\")\n",
        "            f.write(\"‚Ä¢ Refer to chunk_id column to maintain order during analysis\\n\")\n",
        "\n",
        "        print(f\"\\nüìã Summary created: {summary_filename}\")\n",
        "\n",
        "        # List all files created\n",
        "        print(f\"\\n‚úÖ All files saved to Colab directory: {output_dir}\")\n",
        "        print(\"üìÇ Directory contents:\")\n",
        "        for file in os.listdir(output_dir):\n",
        "            file_path = os.path.join(output_dir, file)\n",
        "            file_size = os.path.getsize(file_path)\n",
        "            print(f\"   ‚Ä¢ {file} ({file_size:,} bytes)\")\n",
        "\n",
        "        print(f\"\\nüí° Access your files at: /content/{output_dir}\")\n",
        "        print(\"üí° Use the file browser on the left to navigate to your results\")\n",
        "\n",
        "        return output_filename\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error saving files: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_results_interface():\n",
        "    \"\"\"Create interface for viewing and exporting results\"\"\"\n",
        "\n",
        "    # Instructions\n",
        "    instructions_html = \"\"\"\n",
        "    <div style='background-color: #E7ECEF; padding: 20px; border-radius: 10px; margin: 20px 0; border-left: 5px solid #274C77;'>\n",
        "    <h3 style='color: #274C77; margin-top: 0;'>üìä Analyze Results and Export</h3>\n",
        "    <p><strong>Great job!</strong> Your interview transcript has been successfully chunked. Use the tools below to:</p>\n",
        "    <div style='display: flex; gap: 20px; margin: 15px 0;'>\n",
        "        <div style='flex: 1; background-color: #A3CEF1; padding: 15px; border-radius: 8px; border-left: 4px solid #6096BA;'>\n",
        "            <ul style='color: #274C77; margin: 0;'>\n",
        "                <li>üìà <strong>View Statistics:</strong> Understand your chunking results with detailed analytics</li>\n",
        "                <li>üìä <strong>Generate Visualizations:</strong> See charts and graphs of your data</li>\n",
        "            </ul>\n",
        "        </div>\n",
        "        <div style='flex: 1; background-color: #A3CEF1; padding: 15px; border-radius: 8px; border-left: 4px solid #6096BA;'>\n",
        "            <ul style='color: #274C77; margin: 0;'>\n",
        "                <li>üíæ <strong>Export Files:</strong> Download your chunked data for qualitative analysis software</li>\n",
        "                <li>üéØ <strong>Ready for Analysis:</strong> Import into NVivo, ATLAS.ti, or spreadsheets</li>\n",
        "            </ul>\n",
        "        </div>\n",
        "    </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    # Action buttons\n",
        "    stats_button = widgets.Button(\n",
        "        description='üìä View Detailed Statistics',\n",
        "        disabled=False,\n",
        "        button_style='',\n",
        "        tooltip='Generate comprehensive analysis of your chunks',\n",
        "        icon='chart-bar',\n",
        "        layout=widgets.Layout(width='250px', height='45px'),\n",
        "        style={'button_color': '#A3CEF1', 'font_weight': 'bold'}\n",
        "    )\n",
        "\n",
        "    viz_button = widgets.Button(\n",
        "        description='üìà Generate Visualizations',\n",
        "        disabled=False,\n",
        "        button_style='',\n",
        "        tooltip='Create charts and graphs of your data',\n",
        "        icon='chart-line',\n",
        "        layout=widgets.Layout(width='250px', height='45px'),\n",
        "        style={'button_color': '#A3CEF1', 'font_weight': 'bold'}\n",
        "    )\n",
        "\n",
        "    export_button = widgets.Button(\n",
        "        description='üíæ Export Results',\n",
        "        disabled=False,\n",
        "        button_style='',\n",
        "        tooltip='Download your chunked data files',\n",
        "        icon='download',\n",
        "        layout=widgets.Layout(width='200px', height='45px'),\n",
        "        style={'button_color': '#6096BA', 'font_weight': 'bold'}\n",
        "    )\n",
        "\n",
        "    # Output display\n",
        "    results_output = widgets.Output()\n",
        "\n",
        "    def show_statistics(b):\n",
        "        with results_output:\n",
        "            results_output.clear_output()\n",
        "\n",
        "            if not processing_results:\n",
        "                print(\"‚ùå No processing results available. Please run the chunking process first.\")\n",
        "                return\n",
        "\n",
        "            generate_comprehensive_statistics(\n",
        "                processing_results['dataframe'],\n",
        "                processing_results['original_text']\n",
        "            )\n",
        "\n",
        "    def show_visualizations(b):\n",
        "        with results_output:\n",
        "            results_output.clear_output()\n",
        "\n",
        "            if not processing_results:\n",
        "                print(\"‚ùå No processing results available. Please run the chunking process first.\")\n",
        "                return\n",
        "\n",
        "            create_visualizations(processing_results['dataframe'])\n",
        "\n",
        "    def export_results(b):\n",
        "        with results_output:\n",
        "            results_output.clear_output()\n",
        "\n",
        "            if not processing_results:\n",
        "                print(\"‚ùå No processing results available. Please run the chunking process first.\")\n",
        "                return\n",
        "\n",
        "            save_results(\n",
        "                processing_results['dataframe'],\n",
        "                processing_results['filename'],\n",
        "                processing_results['original_text']\n",
        "            )\n",
        "\n",
        "    # Bind events\n",
        "    stats_button.on_click(show_statistics)\n",
        "    viz_button.on_click(show_visualizations)\n",
        "    export_button.on_click(export_results)\n",
        "\n",
        "    # Layout\n",
        "    analysis_buttons = widgets.HBox([stats_button, viz_button])\n",
        "    export_section = widgets.HBox([export_button])\n",
        "\n",
        "    # Display interface\n",
        "    display(HTML(instructions_html))\n",
        "    display(analysis_buttons)\n",
        "    display(export_section)\n",
        "    display(results_output)\n",
        "\n",
        "    return {\n",
        "        'stats_button': stats_button,\n",
        "        'viz_button': viz_button,\n",
        "        'export_button': export_button,\n",
        "        'output': results_output\n",
        "    }\n",
        "\n",
        "# Initialize results interface\n",
        "print(\"üìä Results Analysis and Export Interface Ready\")\n",
        "print(\"üëÜ Process your transcript above, then analyze and export your results below!\")\n",
        "\n",
        "# Create and display results interface\n",
        "results_interface = create_results_interface()\n",
        "\n",
        "# Final summary\n",
        "print(\"\\\\n\" + \"=\" * 70)\n",
        "print(\"üéâ INTERVIEW TRANSCRIPT SEMANTIC CHUNKER - READY FOR USE!\")\n",
        "print(\"=\" * 70)\n",
        "print(\"üìã Complete Workflow:\")\n",
        "print(\"   1. ‚úÖ Configure your chunking parameters\")\n",
        "print(\"   2. ‚úÖ Upload your interview transcript files\")\n",
        "print(\"   3. ‚úÖ Run the semantic chunking process\")\n",
        "print(\"   4. ‚úÖ Analyze results and export for qualitative analysis\")\n",
        "print()\n",
        "print(\"üí° Tips for Best Results:\")\n",
        "print(\"   ‚Ä¢ Start with default settings and adjust based on your needs\")\n",
        "print(\"   ‚Ä¢ Ensure speaker labels are consistent (Q:, A:, Name:)\")\n",
        "print(\"   ‚Ä¢ Test different similarity thresholds for optimal chunking\")\n",
        "print(\"   ‚Ä¢ Export in Excel format for rich metadata and multiple sheets\")\n",
        "print()\n",
        "print(\"üöÄ Ready to transform your interview data into analyzable chunks!\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "id": "14BCw3dEC7Pe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}